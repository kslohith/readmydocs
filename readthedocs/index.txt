[Document(id_='802f4c1c-4aab-4f0b-a56c-9eca3c2176c4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='411b16914f5b7126134acbf6be32de469204e231c3e6555e3d5017ee2536c3ce', text='EVA DB\nEVA DB\nMay 18, 2023OVERVIEW\n1 What is EVA? 3\n2 Key Features 5\n3 Next Steps 7\n4 Illustrative EVA Applications 9\n4.1 Traffic Analysis Application using Object Detection Model . . . . . . . . . . . . . . . . . . . . . . 9\n4.2 MNIST Digit Recognition using Image Classification Model . . . . . . . . . . . . . . . . . . . . . 9\n4.3 Movie Analysis Application using Face Detection + Emotion Classification Models . . . . . . . . . 9\n5 Community 11\n6 EVA AI-Relational Database System 13\n6.1 Usability and Application Maintainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n6.2 GPU Cost and Human Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n7 Getting Started 15\n7.1 Part 1: Install EVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n7.2 Launch EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n7.3 Part 2: Start a Jupyter Notebook Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n7.4 Part 3: Register an user-defined function (UDF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n7.5 Part 5: Start a Command Line Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n8 Start EVA Server 19\n8.1 Launch EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n9 MNIST TUTORIAL 21\n9.1 Start EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n9.2 Downloading the videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n9.3 Upload the video for analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n9.4 Visualize Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n9.5 Create an user-defined function (UDF) for analyzing the frames . . . . . . . . . . . . . . . . . . . . 22\n9.6 Run the Image Classification UDF on video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n9.7 Visualize output of query on the video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n10 Object Detection Tutorial 25\n10.1 Start EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n10.2 Download the Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n10.3 Load the surveillance videos for analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n10.4 Visualize Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\ni10.5 Register YOLO Object Detector an an User-Defined Function (UDF) in EVA . . . . . . . . . . . . . 26\n10.6 Run Object Detector on the video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n10.7 Visualizing output of the Object Detector on the video . . . . . . . . . . . . . . . . . . . . . . . . . 29\n10.8 Dropping an User-Defined Function (UDF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n11 EMOTION ANALYSIS 33\n11.1 Start EVA Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n11.2 Video Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n11.3 Adding the video file to EVADB for analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n11.4 Visualize Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n11.5 Create an user-defined function(UDF) for analyzing the frames . . . . . . . . . . . . . . . . . . . . . 34\n11.6 Run the Face Detection UDF on video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n11.7 Run the Emotion Detection UDF on the outputs of the Face Detection UDF . . . . . . . . . . . . . . 35\n12 Image Segmentation Tutorial 39\n12.1 Start EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n12.2 Download the Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n12.3 Load sample video from DAVIS dataset for analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n12.4 Visualize Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n12.5 Register Hugging Face Segmentation Model as an User-Defined Function (UDF) in EVA . . . . . . . 40\n12.6 Run Image Segmentation on the video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n12.7 Visualizing output of the Image Segmenter on the video . . . . . . . . . . . . . . . . . . . . . . . . 41\n12.8 Dropping an User-Defined Function (UDF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n13 ChatGPT Tutorial 45\n13.1 Start EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n13.2 Download News Video and ChatGPT UDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n13.3 Visualize Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n13.4 Set your OpenAI API key here . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n13.5 Run the ChatGPT UDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n13.6 Check if it works on an SNL Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n14 Similarity search for motif mining 51\n14.1 Start EVA server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n14.2 Download reddit dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n14.3 Load all images into evadb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n14.4 Register a SIFT FeatureExtractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n14.5 Image-level similarity search pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n14.6 Object-level similarity search pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n14.7 Combine the scores from image level and object level similarity to show similar images . . . . . . . 56\n15 EVA Query Language Reference 57\n15.1 LOAD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n15.2 SELECT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n15.3 EXPLAIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n15.4 SHOW . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n15.5 CREATE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n15.6 DROP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n15.7 INSERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n15.8 DELETE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n15.9 RENAME . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n16 User-Defined Functions 63\n16.1 Part 1: Writing a custom UDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n16.2 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nii16.3 Forward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n16.4 Part 2: Registering and using the UDF in EVA Queries . . . . . . . . . . . . . . . . . . . . . . . . . 65\n16.5 Ultralytics Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n16.6 HuggingFace Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n16.7 OpenAI Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n16.8 User-Defined Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n17 IO Descriptors 73\n17.1 NumpyArray . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n17.2 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n17.3 PyTorchTensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n17.4 PandasDataframe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n18 Configure GPU 75\n19 EVA Internals 77\n19.1 Path of a Query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n19.2 Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n20 Contributing 79\n20.1 Setting up the Development Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n20.2 Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n20.3 Submitting a Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n20.4 Code Style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n20.5 Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n20.6 Architecture Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n20.7 Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n21 Debugging 83\n21.1 Setup debugger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n21.2 Alternative: Manually Setup Debugger for EVA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n22 Extending EVA 87\n22.1 Command Handler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n22.2 1. Parser . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n22.3 2. Statement To Plan Converter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n22.4 3. Plan Generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n22.5 4. Plan Executor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n22.6 Additional Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n23 EVA Release Guide 93\n23.1 Part 1: Before You Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n23.2 Part 2: Release Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n24 Packaging 97\n24.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n24.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\niiiivEVA DB\nDatabase system for building simpler and faster AI-powered applications.\nOVERVIEW 1EVA DB\n2 OVERVIEWCHAPTER\nONE\nWHAT IS EVA?\nEVAisan open-sourceAI-relationaldatabasewithfirst-classsupportfordeeplearningmodels . Itaimstosupport\nAI-powereddatabaseapplicationsthatoperateonbothstructured(tables)andunstructureddata(videos,text,podcasts,\nPDFs, etc.) with deep learning models.\nEVA accelerates AI pipelines using a collection of optimizations inspired by relational database systems including\nfunction caching, sampling, and cost-based operator reordering. It comes with a wide range of models for analyzing\nunstructured data including image classification, object detection, OCR, face detection, etc. It is fully implemented in\nPython, and licensed under the Apache license.\nEVA supports a AI-oriented query language for analysing unstructured data. Here are some illustrative applications:\n•Using ChatGPT to ask questions based on videos\n•Analysing traffic flow at an intersection\n•Examining the emotion palette of actors in a movie\n•Finding similar images on Reddit\n•Classifying images based on their content\n•Image Segmentation using Hugging Face\n•Recognizing license plates\n•Analysing toxicity of social media memes\nIfyouarewonderingwhyyoumightneedaAI-Relationaldatabasesystem,startwithpageonAI-RelationalDatabase\nSystems. It describes how EVA lets users easily make use of deep learning models and how they can reduce money\nspent on inference on large image or video datasets.\nThe Getting Started page shows how you can use EVA for different computer vision tasks, and how you can easily\nextend EVA to support your custom deep learning model in the form of user-defined functions.\nTheUserGuidessectioncontainsJupyterNotebooksthatdemonstratehowtousevariousfeaturesofEVA.Eachnote-\nbook includes a link to Google Colab, where you can run the code by yourself.\n3EVA DB\n4 Chapter 1. What is EVA?CHAPTER\nTWO\nKEY FEATURES\n1. With EVA, you can easily combine SQL and deep learning models to build next-generation database ap-\nplications . EVA treats deep learning models as functions similar to traditional SQL functions like SUM().\n2. EVA is extensible by design . You can write an user-defined function (UDF) that wraps around your custom\ndeeplearningmodel. Infact,allthebuilt-inmodelsthatareincludedinEVAarewrittenasuser-definedfunctions.\n3. EVAcomeswithacollectionof built-insampling,caching,andfilteringoptimizations inspiredbyrelational\ndatabase systems. These optimizations help speed up queries on large datasets and save money spent on\nmodel inference .\n5EVA DB\n6 Chapter 2. Key FeaturesCHAPTER\nTHREE\nNEXT STEPS\nGetting Started A step-by-step guide to installing EVA and running queries\nQuery Language List of all the query commands supported by EVA\nUser Defined Functions A step-by-step tour of registering a user defined function that wraps around a\ncustom deep learning model\n7EVA DB\n8 Chapter 3. Next StepsCHAPTER\nFOUR\nILLUSTRATIVE EVA APPLICATIONS\n4.1 Traffic Analysis Application using Object Detection Model\n4.2 MNIST Digit Recognition using Image Classification Model\n4.3 MovieAnalysisApplicationusingFaceDetection+EmotionClas-\nsification Models\n9EVA DB\n10 Chapter 4. Illustrative EVA ApplicationsCHAPTER\nFIVE\nCOMMUNITY\nJoin the EVA community on Slack to ask questions and to share your ideas for improving EVA.\n11EVA DB\n12 Chapter 5. CommunityCHAPTER\nSIX\nEVA AI-RELATIONAL DATABASE SYSTEM\nOver the last decade, deep learning models have radically changed the world of computer vision and natural language\nprocessing. Theyareaccurateonavarietyoftasksrangingfromimageclassificationtoquestionanswering. However,\nthere are two challenges that prevent a lot of users from benefiting from these models.\n6.1 Usability and Application Maintainability\nTo use a vision or language model, the user must do a lot of imperative programming across low-level libraries, like\nOpenCV, PyTorch, and Hugging Face. This is a tedious process that often leads to a complex program or Jupyter\nNotebook that glues together these libraries to accomplish the given task. This programming complexity prevents a\nlot of people who are experts in other domains from benefiting from these models .\nHistorically,databasesystemshavebeensuccessfulbecausethe querylanguageissimpleenough initsbasicstructure\nthatuserswithoutpriorexperienceareabletolearnausablesubsetofthelanguageontheirfirstsitting. EVAsupports\na declarative SQL-like query language, called EVAQL, that is designed to make it easier for users to leverage these\nmodels. Withthisquerylanguage,theusermay composemultiplemodelsinasinglequery toaccomplishcomplicated\ntasks with minimal programming .\nHereisaillustrativequerythatexaminestheemotionsofactorsinamoviebyleveragingmultipledeeplearningmodels\nthat take care of detecting faces and analyzing the emotions of the detected bounding boxes:\nSELECTid, bbox, EmotionDetector(Crop( data, bbox))\nFROMInterstellar\nJOIN LATERAL UNNEST (FaceDetector( data))ASFace(bbox, conf)\nWHEREid < 15;\nBy using a declarative language, the complexity of the program or Jupyter Notebook is significantly reduced. This in\nturn leads to more maintainable code that allows users to build on top of each other’s queries.\n6.2 GPU Cost and Human Time\nFrom a cost standpoint, it is very expensive to run these deep learning models on large image or video datasets. For\nexample,thestate-of-the-artobjectdetectionmodeltakesmultipleGPU-decadestoprocessjustayear’sworthofvideos\nfrom a single traffic monitoring camera. Besides the money spent on hardware, this also increases the time that the\nuser spends waiting for the model inference process to finish.\nEVAautomatically optimizesthequeriesto reduceinferencecostandqueryexecutiontime usingitsCascades-style\nquery optimizer. EVA’s optimizer is tailored for video analytics. The Cascades-style extensible query optimization\nframeworkhasworkedverywellforseveraldecadesinSQLdatabasesystems. Queryoptimizationisoneofthesigna-\nturecomponentsofdatabasesystems—thebridgethatconnectsthedeclarativequerylanguagetoefficientexecution.\n13EVA DB\n14 Chapter 6. EVA AI-Relational Database SystemCHAPTER\nSEVEN\nGETTING STARTED\n7.1 Part 1: Install EVA\nEVA supports Python (versions >= 3.7). To install EVA, we recommend using the pip package manager:\npip install evadb\n7.2 Launch EVA server\nEVA is based on a client-server architecture. To launch the EVA server, run the following command on the terminal:\neva_server &\n7.3 Part 2: Start a Jupyter Notebook Client\nHere is an illustrative Jupyter notebook focusing on MNIST image classification using EVA. The notebook works on\nGoogle Colab.\n7.3.1 Connect to the EVA server\nTo connect to the EVA server in the notebook, use the following Python code:\n# allow nested asyncio calls for client to connect with server\nimport nest_asyncio\nnest_asyncio.apply()\nfrom eva.server.db_api import connect\n# hostname and port of the server where EVA is running\nconnection = connect(host = \'0.0.0.0 \', port = 8803)\n# cursor allows the notebook client to send queries to the server\ncursor = connection.cursor()\n15EVA DB\n7.3.2 Load video for analysis\nDownload the MNIST video for analysis.\n!wget -nc https://www.dropbox.com/s/yxljxz6zxoqu54v/mnist.mp4\nUse the LOAD statement is used to load a video onto a table in EVA server.\ncursor.execute( \'LOAD VIDEO "mnist.mp4" INTO MNISTVideoTable; \')\nresponse = cursor.fetch_all()\nprint(response)\n7.4 Part 3: Register an user-defined function (UDF)\nUser-definedfunctionsallowustocombineSQLwithdeeplearningmodels. Thesefunctionswraparounddeeplearn-\ning models.\nDownload the user-defined function for classifying MNIST images.\n!wget -nc https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/apps/\n˓→mnist/eva_mnist_udf.py\ncursor.execute("""CREATE UDF IF NOT EXISTS MnistCNN\nINPUT (data NDARRAY (3, 28, 28))\nOUTPUT (label TEXT(2))\nTYPE Classification\nIMPL \'eva_mnist_udf.py \';\n""")\nresponse = cursor.fetch_all()\nprint(response)\n7.4.1 Run a query using the newly registered UDF!\ncursor.execute("""SELECT data, MnistCNN(data).label\nFROM MNISTVideoTable\nWHERE id = 30;""")\nresponse = cursor.fetch_all()\n7.4.2 Visualize the output\nThe output of the query is visualized in the notebook.\n16 Chapter 7. Getting StartedEVA DB\n7.5 Part 5: Start a Command Line Client\nBesides the notebook interface, EVA also exports a command line interface for querying the server. This interface\nallows for quick querying from the terminal:\n>>> eva_client\neva= # LOAD VIDEO "mnist.mp4" INTO MNISTVid;\n@status: ResponseStatus.SUCCESS\n@batch:\n0 Video successfully added at location: mnist.p4\n@query_time: 0.045\neva= # SELECT id, data FROM MNISTVid WHERE id < 1000;\n@status: ResponseStatus.SUCCESS\n@batch:\nmnistvid.id mnistvid.data\n0 0 [[[ 0 2 0] \\n[0 0 0] \\n...\n1 1 [[[ 2 2 0] \\n[1 1 0] \\n...\n2 2 [[[ 2 2 0] \\n[1 2 2] \\n...\n.. ...\n997 997 [[[ 0 2 0] \\n[0 0 0] \\n...\n998 998 [[[ 0 2 0] \\n[0 0 0] \\n...\n999 999 [[[ 2 2 0] \\n[1 1 0] \\n...\n[1000 rows x 2 columns]\n@query_time: 0.216\neva= # exit\n7.5. Part 5: Start a Command Line Client 17EVA DB\n18 Chapter 7. Getting StartedCHAPTER\nEIGHT\nSTART EVA SERVER\n8.1 Launch EVA server\nWe use this notebook for launching the EVA server.\n## Install EVA package if needed\n%pipinstall "evadb" --quiet\nimport os\nimport time\nfrom psutil import process_iter\nfrom signal import SIGTERM\nimport re\nimport itertools\ndefshell(command):\nprint(command)\nos.system(command)\ndefstop_eva_server():\nforproc inprocess_iter():\nifproc.name() == "eva_server":\nproc.send_signal(SIGTERM)\ndefis_eva_server_running():\nforproc inprocess_iter():\nifproc.name() == "eva_server":\nreturn True\nreturn False\ndeflaunch_eva_server():\n# Stop EVA server if it is running\n# stop_eva_server()\nos.environ[ \'GPU_DEVICES \'] = \'0\'\n# Start EVA server\nshell("nohup eva_server > eva.log 2>&1 &")\nlast_few_lines_count = 3\n(continues on next page)\n19EVA DB\n(continued from previous page)\ntry:\nwithopen( \'eva.log \',\'r\')asf:\nforlines initertools.zip_longest(*[f]*last_few_lines_count):\nprint(lines)\nexceptFileNotFoundError:\npass\n# Wait for server to start\ntime.sleep(10)\ndefconnect_to_server():\nfrom eva.server.db_api import connect\nimport nest_asyncio\nnest_asyncio.apply()\nstatus = is_eva_server_running()\nifstatus == False:\nlaunch_eva_server()\n# Connect client with server\nconnection = connect(host = \'127.0.0.1 \', port = 8803)\ncursor = connection.cursor()\nreturncursor\n# Launch server\nlaunch_eva_server()\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server --port 8803 > eva.log 2>&1 &\n20 Chapter 8. Start EVA ServerCHAPTER\nNINE\nMNIST TUTORIAL\n9.1 Start EVA server\nWe are reusing the start server notebook for launching the EVA server.\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nFile ‘00-start-eva-server.ipynb’ already there; not retrieving.\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server > eva.log 2>&1 &\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n9.2 Downloading the videos\n# Getting MNIST as a video\n!wget -nc https://www.dropbox.com/s/yxljxz6zxoqu54v/mnist.mp4\n# Getting a udf\n!wget -nc https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/apps/\n˓→mnist/eva_mnist_udf.py\nFile ‘mnist.mp4’ already there; not retrieving.\nFile ‘eva_mnist_udf.py’ already there; not retrieving.\n21EVA DB\n9.3 Upload the video for analysis\ncursor.execute( \'DROP TABLE IF EXISTS MNISTVid \')\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute("LOAD VIDEO \'mnist.mp4 \'INTO MNISTVid")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded VIDEO: 1\n9.4 Visualize Video\nfrom IPython.display import Video\nVideo("mnist.mp4", embed= True)\n<IPython.core.display.Video object>\n9.5 Create an user-defined function (UDF) for analyzing the frames\ncursor.execute("""CREATE UDF IF NOT EXISTS\nMnistCNN\nINPUT (data NDARRAY (3, 28, 28))\nOUTPUT (label TEXT(2))\nTYPE Classification\nIMPL \'eva_mnist_udf.py \'\n""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF MnistCNN successfully added to the database.\n9.6 Run the Image Classification UDF on video\ncursor.execute("""SELECT data, MnistCNN(data).label\nFROM MNISTVid\nWHERE id = 30 OR id = 50 OR id = 70 OR id = 0 OR id = 140""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nmnistvid.data mnistcnn.label\n0 [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ... 6\n(continues on next page)\n22 Chapter 9. MNIST TUTORIALEVA DB\n(continued from previous page)\n1 [[[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], ... 2\n2 [[[13, 13, 13], [2, 2, 2], [2, 2, 2], [13, 13,... 3\n3 [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ... 7\n4 [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ... 5\n9.7 Visualize output of query on the video\n# !pip install matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n# create figure (fig), and array of axes (ax)\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=[6,8])\ndf = response.batch.frames\nforaxi inax.flat:\nidx = np.random.randint(len(df))\nimg = df[ \'mnistvid.data \'].iloc[idx]\nlabel = df[ \'mnistcnn.label \'].iloc[idx]\naxi.imshow(img)\naxi.set_title(f \'label: {label }\')\nplt.show()\n../_readthedocs/jupyter_execute/01-mnist_15_0.png\n9.7. Visualize output of query on the video 23EVA DB\n24 Chapter 9. MNIST TUTORIALCHAPTER\nTEN\nOBJECT DETECTION TUTORIAL\n10.1 Start EVA server\nWe are reusing the start server notebook for launching the EVA server.\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nFile ‘00-start-eva-server.ipynb’ already there; not retrieving.\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server > eva.log 2>&1 &\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n10.2 Download the Videos\n# Getting the video files\n!wget -nc "https://www.dropbox.com/s/k00wge9exwkfxz6/ua_detrac.mp4?raw=1" -O ua_detrac.\n˓→mp4\nFile ‘ua_detrac.mp4’ already there; not retrieving.\n25EVA DB\n10.3 Load the surveillance videos for analysis\n10.3.1 We use regular expression to load all the videos into the table\ncursor.execute( \'DROP TABLE IF EXISTS ObjectDetectionVideos \')\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute( \'LOAD VIDEO "ua_detrac.mp4" INTO ObjectDetectionVideos; \')\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded VIDEO: 1\n10.4 Visualize Video\nfrom IPython.display import Video\nVideo("ua_detrac.mp4", embed= True)\n<IPython.core.display.Video object>\n10.5 Register YOLO Object Detector an an User-Defined Function\n(UDF) in EVA\ncursor.execute("""\nCREATE UDF IF NOT EXISTS Yolo\nTYPE ultralytics\n\'model \' \'yolov8m.pt \';\n""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF Yolo already exists, nothing added.\n26 Chapter 10. Object Detection TutorialEVA DB\n10.6 Run Object Detector on the video\ncursor.execute("""SELECT id, Yolo(data)\nFROM ObjectDetectionVideos\nWHERE id < 20""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nobjectdetectionvideos.id \\\n0 0\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n7 7\n8 8\n9 9\n10 10\n11 11\n12 12\n13 13\n14 14\n15 15\n16 16\n17 17\n18 18\n19 19\nyolo.labels \\\n0 [car, car, car, car, car, car, person, car, ca...\n1 [car, car, car, car, car, car, car, car, car, ...\n2 [car, car, car, car, car, car, car, person, ca...\n3 [car, car, car, car, car, car, car, car, car, ...\n4 [car, car, car, car, car, car, car, car, car, ...\n5 [car, car, car, car, car, car, person, car, ca...\n6 [car, car, car, car, car, car, car, person, ca...\n7 [car, car, car, car, car, car, car, car, car, ...\n8 [car, car, car, car, car, car, person, car, ca...\n9 [car, car, car, car, car, car, car, person, ca...\n10 [car, car, car, car, car, car, car, person, ca...\n11 [car, car, car, car, car, car, person, car, ca...\n12 [car, car, car, car, car, car, car, person, ca...\n13 [car, car, car, car, car, car, person, car, ca...\n14 [car, car, car, car, car, car, person, car, ca...\n15 [car, car, car, car, car, car, person, car, ca...\n16 [car, car, car, car, car, car, car, person, ca...\n17 [car, car, car, car, car, car, car, person, ca...\n18 [car, car, car, car, car, car, car, person, mo...\n19 [car, car, car, car, car, person, car, car, ca...\n(continues on next page)\n10.6. Run Object Detector on the video 27EVA DB\n(continued from previous page)\nyolo.bboxes \\\n0 [[829.0, 277.0, 960.0, 360.0], [615.0, 216.0, ...\n1 [[832.0, 278.0, 960.0, 361.0], [616.0, 216.0, ...\n2 [[836.0, 279.0, 960.0, 362.0], [618.0, 216.0, ...\n3 [[839.0, 280.0, 960.0, 363.0], [619.0, 217.0, ...\n4 [[843.0, 281.0, 960.0, 364.0], [621.0, 218.0, ...\n5 [[847.0, 282.0, 960.0, 363.0], [623.0, 218.0, ...\n6 [[851.0, 283.0, 959.0, 360.0], [625.0, 219.0, ...\n7 [[855.0, 284.0, 960.0, 357.0], [626.0, 220.0, ...\n8 [[859.0, 285.0, 960.0, 357.0], [628.0, 221.0, ...\n9 [[863.0, 286.0, 960.0, 357.0], [630.0, 222.0, ...\n10 [[632.0, 223.0, 744.0, 284.0], [867.0, 287.0, ...\n11 [[871.0, 289.0, 960.0, 356.0], [634.0, 223.0, ...\n12 [[636.0, 223.0, 750.0, 287.0], [875.0, 290.0, ...\n13 [[171.0, 409.0, 291.0, 539.0], [637.0, 224.0, ...\n14 [[174.0, 405.0, 294.0, 538.0], [885.0, 291.0, ...\n15 [[888.0, 293.0, 960.0, 355.0], [177.0, 400.0, ...\n16 [[893.0, 293.0, 960.0, 355.0], [180.0, 396.0, ...\n17 [[182.0, 392.0, 296.0, 519.0], [897.0, 294.0, ...\n18 [[901.0, 295.0, 960.0, 356.0], [647.0, 225.0, ...\n19 [[648.0, 226.0, 770.0, 293.0], [906.0, 297.0, ...\nyolo.scores\n0 [0.91, 0.86, 0.85, 0.83, 0.76, 0.73, 0.72, 0.7...\n1 [0.92, 0.85, 0.84, 0.83, 0.78, 0.76, 0.76, 0.7...\n2 [0.92, 0.84, 0.84, 0.82, 0.81, 0.75, 0.73, 0.7...\n3 [0.91, 0.84, 0.82, 0.8, 0.8, 0.75, 0.74, 0.72,...\n4 [0.9, 0.85, 0.83, 0.8, 0.76, 0.73, 0.72, 0.72,...\n5 [0.89, 0.86, 0.84, 0.8, 0.78, 0.74, 0.72, 0.72...\n6 [0.89, 0.87, 0.85, 0.81, 0.79, 0.73, 0.72, 0.7...\n7 [0.9, 0.87, 0.84, 0.83, 0.83, 0.79, 0.73, 0.67...\n8 [0.89, 0.88, 0.83, 0.82, 0.79, 0.71, 0.68, 0.6...\n9 [0.88, 0.87, 0.84, 0.82, 0.8, 0.75, 0.74, 0.74...\n10 [0.88, 0.88, 0.85, 0.82, 0.8, 0.79, 0.76, 0.71...\n11 [0.9, 0.9, 0.85, 0.8, 0.79, 0.77, 0.69, 0.68, ...\n12 [0.9, 0.88, 0.83, 0.81, 0.78, 0.78, 0.78, 0.67...\n13 [0.9, 0.89, 0.89, 0.83, 0.81, 0.81, 0.72, 0.71...\n14 [0.9, 0.89, 0.88, 0.84, 0.82, 0.81, 0.75, 0.72...\n15 [0.89, 0.88, 0.87, 0.84, 0.82, 0.78, 0.76, 0.7...\n16 [0.88, 0.88, 0.87, 0.82, 0.81, 0.76, 0.75, 0.7...\n17 [0.9, 0.89, 0.87, 0.83, 0.82, 0.78, 0.72, 0.69...\n18 [0.88, 0.88, 0.83, 0.82, 0.8, 0.78, 0.75, 0.7,...\n19 [0.89, 0.87, 0.81, 0.8, 0.78, 0.77, 0.73, 0.72...\n28 Chapter 10. Object Detection TutorialEVA DB\n10.7 Visualizing output of the Object Detector on the video\nimport cv2\nfrom pprint import pprint\nfrom matplotlib import pyplot asplt\ndefannotate_video(detections, input_video_path, output_video_path):\ncolor1=(207, 248, 64)\ncolor2=(255, 49, 49)\nthickness=4\nvcap = cv2.VideoCapture(input_video_path)\nwidth = int(vcap.get(3))\nheight = int(vcap.get(4))\nfps = vcap.get(5)\nfourcc = cv2.VideoWriter_fourcc( \'m\',\'p\',\'4\',\'v\')#codec\nvideo=cv2.VideoWriter(output_video_path, fourcc, fps, (width,height))\nframe_id = 0\n# Capture frame-by-frame\n# ret = 1 if the video is captured; frame is the image\nret, frame = vcap.read()\nwhileret:\ndf = detections\ndf = df[[ \'yolo.bboxes \',\'yolo.labels \']][df.index == frame_id]\nifdf.size:\ndfLst = df.values.tolist()\nforbbox, label inzip(dfLst[0][0], dfLst[0][1]):\nx1, y1, x2, y2 = bbox\nx1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n# object bbox\nframe=cv2.rectangle(frame, (x1, y1), (x2, y2), color1, thickness)\n# object label\ncv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,␣\n˓→color1, thickness)\n# frame label\ncv2.putText(frame, \'Frame ID: \'+ str(frame_id), (700, 500), cv2.FONT_\n˓→HERSHEY_SIMPLEX, 1.2, color2, thickness)\nvideo.write(frame)\n# Stop after twenty frames (id < 20 in previous query)\nifframe_id == 20:\nbreak\n# Show every fifth frame\nifframe_id % 5 == 0:\nplt.imshow(frame)\nplt.show()\nframe_id+=1\n(continues on next page)\n10.7. Visualizing output of the Object Detector on the video 29EVA DB\n(continued from previous page)\nret, frame = vcap.read()\nvideo.release()\nvcap.release()\nfrom ipywidgets import Video, Image\ninput_path = \'ua_detrac.mp4 \'\noutput_path = \'video.mp4 \'\ndataframe = response.as_df()\nannotate_video(dataframe, input_path, output_path)\nVideo.from_file(output_path)\n../_readthedocs/jupyter_execute/02-object-detection_17_0.png\n../_readthedocs/jupyter_execute/02-object-detection_17_1.png\n../_readthedocs/jupyter_execute/02-object-detection_17_2.png\n../_readthedocs/jupyter_execute/02-object-detection_17_3.png\nVideo(value=b \'\\x00\\x00\\x00\\x1cftypisom\\x00\\x00\\x02\\x00isomiso2mp41\\x00\\x00\\x00\\x08free\\\n˓→x00\\tI\\x95mdat\\x00\\x00\\...\n30 Chapter 10. Object Detection TutorialEVA DB\n10.8 Dropping an User-Defined Function (UDF)\ncursor.execute("DROP UDF IF EXISTS Yolo;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF Yolo successfully dropped\n10.8. Dropping an User-Defined Function (UDF) 31EVA DB\n32 Chapter 10. Object Detection TutorialCHAPTER\nELEVEN\nEMOTION ANALYSIS\n11.1 Start EVA Server\nWe are reusing the start server notebook for launching the EVA server\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nFile ‘00-start-eva-server.ipynb’ already there; not retrieving.\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server > eva.log 2>&1 &\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n11.2 Video Files\ngetting some video files to test\n# A video of a happy person\n!wget -nc "https://www.dropbox.com/s/gzfhwmib7u804zy/defhappy.mp4?raw=1" -O defhappy.mp4\n# Adding Emotion detection\n!wget -nc https://raw.githubusercontent.com/georgia-tech-db/eva/master/eva/udfs/emotion_\n˓→detector.py\n# Adding Face Detector\n!wget -nc https://raw.githubusercontent.com/georgia-tech-db/eva/master/eva/udfs/face_\n˓→detector.py(continues on next page)\n33EVA DB\n(continued from previous page)\nFile ‘defhappy.mp4’ already there; not retrieving.\nFile ‘emotion_detector.py’ already there; not retrieving.\nFile ‘face_detector.py’ already there; not retrieving.\n11.3 Adding the video file to EVADB for analysis\ncursor.execute( \'DROP TABLE IF EXISTS HAPPY \')\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute( \'LOAD VIDEO "defhappy.mp4" INTO HAPPY \')\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded VIDEO: 1\n11.4 Visualize Video\nfrom IPython.display import Video\nVideo("defhappy.mp4", height=450, width=800, embed= True)\n<IPython.core.display.Video object>\n11.5 Create an user-defined function(UDF) for analyzing the frames\ncursor.execute("""CREATE UDF IF NOT EXISTS EmotionDetector\nINPUT (frame NDARRAY UINT8(3, ANYDIM, ANYDIM))\nOUTPUT (labels NDARRAY STR(ANYDIM), scores NDARRAY FLOAT32(ANYDIM))\nTYPE Classification IMPL \'emotion_detector.py \';\n""")\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute("""CREATE UDF IF NOT EXISTS FaceDetector\nINPUT (frame NDARRAY UINT8(3, ANYDIM, ANYDIM))\nOUTPUT (bboxes NDARRAY FLOAT32(ANYDIM, 4),\nscores NDARRAY FLOAT32(ANYDIM))\nTYPE FaceDetection\nIMPL \'face_detector.py \';\n""")\n(continues on next page)\n34 Chapter 11. EMOTION ANALYSISEVA DB\n(continued from previous page)\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF FaceDetector successfully added to the dat...\n11.6 Run the Face Detection UDF on video\ncursor.execute("""SELECT id, FaceDetector(data)\nFROM HAPPY WHERE id<10""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nhappy.id facedetector.bboxes \\\n0 0 [[502, 94, 762, 435], [238, 296, 325, 398]]\n1 1 [[501, 96, 763, 435]]\n2 2 [[504, 97, 766, 437]]\n3 3 [[498, 90, 776, 446]]\n4 4 [[496, 99, 767, 444]]\n5 5 [[499, 87, 777, 448], [236, 305, 324, 407]]\n6 6 [[500, 89, 778, 449]]\n7 7 [[501, 89, 781, 452]]\n8 8 [[503, 90, 783, 450]]\n9 9 [[508, 87, 786, 447]]\nfacedetector.scores\n0 [0.99990165, 0.79820246]\n1 [0.999918]\n2 [0.9999138]\n3 [0.99996686]\n4 [0.9999982]\n5 [0.9999136, 0.8369736]\n6 [0.9999131]\n7 [0.9999124]\n8 [0.99994683]\n9 [0.999949]\n11.7 Run the Emotion Detection UDF on the outputs of the Face De-\ntection UDF\ncursor.execute("""SELECT id, bbox, EmotionDetector(Crop(data, bbox))\nFROM HAPPY JOIN LATERAL UNNEST(FaceDetector(data)) AS Face(bbox,␣\n˓→conf)\nWHERE id < 15;""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n11.6. Run the Face Detection UDF on video 35EVA DB\nhappy.id Face.bbox emotiondetector.labels \\\n0 0 [502, 94, 762, 435] happy\n1 0 [238, 296, 325, 398] neutral\n2 1 [501, 96, 763, 435] happy\n3 2 [504, 97, 766, 437] happy\n4 3 [498, 90, 776, 446] happy\n5 4 [496, 99, 767, 444] happy\n6 5 [499, 87, 777, 448] happy\n7 5 [236, 305, 324, 407] neutral\n8 6 [500, 89, 778, 449] happy\n9 7 [501, 89, 781, 452] happy\n10 8 [503, 90, 783, 450] happy\n11 9 [508, 87, 786, 447] happy\n12 10 [505, 86, 788, 452] happy\n13 10 [235, 309, 322, 411] neutral\n14 11 [514, 85, 790, 454] happy\n15 12 [514, 86, 790, 454] happy\n16 13 [515, 87, 790, 454] happy\n17 14 [516, 86, 792, 455] happy\nemotiondetector.scores\n0 0.999642\n1 0.780949\n2 0.999644\n3 0.999668\n4 0.999654\n5 0.999649\n6 0.999710\n7 0.760779\n8 0.999671\n9 0.999671\n10 0.999689\n11 0.999691\n12 0.999729\n13 0.407872\n14 0.999745\n15 0.999729\n16 0.999718\n17 0.999739\nimport cv2\nfrom pprint import pprint\nfrom matplotlib import pyplot asplt\ndefannotate_video(detections, input_video_path, output_video_path):\ncolor1=(207, 248, 64)\ncolor2=(255, 49, 49)\nthickness=4\nvcap = cv2.VideoCapture(input_video_path)\nwidth = int(vcap.get(3))\nheight = int(vcap.get(4))\n(continues on next page)\n36 Chapter 11. EMOTION ANALYSISEVA DB\n(continued from previous page)\nfps = vcap.get(5)\nfourcc = cv2.VideoWriter_fourcc( \'m\',\'p\',\'4\',\'v\')#codec\nvideo=cv2.VideoWriter(output_video_path, fourcc, fps, (width,height))\nframe_id = 0\n# Capture frame-by-frame\n# ret = 1 if the video is captured; frame is the image\nret, frame = vcap.read()\nwhileret:\ndf = detections\ndf = df[[ \'Face.bbox \',\'emotiondetector.labels \',\'emotiondetector.scores \']][df.\n˓→index == frame_id]\nifdf.size:\nx1, y1, x2, y2 = df[ \'Face.bbox \'].values[0]\nlabel = df[ \'emotiondetector.labels \'].values[0]\nscore = df[ \'emotiondetector.scores \'].values[0]\nx1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n# object bbox\nframe=cv2.rectangle(frame, (x1, y1), (x2, y2), color1, thickness)\n# object label\ncv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color1,\n˓→thickness)\n# object score\ncv2.putText(frame, str(round(score, 5)), (x1+120, y1-10), cv2.FONT_HERSHEY_\n˓→SIMPLEX, 0.9, color1, thickness)\n# frame label\ncv2.putText(frame, \'Frame ID: \'+ str(frame_id), (700, 500), cv2.FONT_\n˓→HERSHEY_SIMPLEX, 1.2, color2, thickness)\nvideo.write(frame)\n# Show every fifth frame\nifframe_id % 5 == 0:\nplt.imshow(frame)\nplt.show()\nframe_id+=1\nret, frame = vcap.read()\nvideo.release()\nvcap.release()\nfrom ipywidgets import Video, Image\ninput_path = \'defhappy.mp4 \'\noutput_path = \'video.mp4 \'\ndataframe = response.as_df()\nannotate_video(dataframe, input_path, output_path)\n11.7. Run the Emotion Detection UDF on the outputs of the Face Detection UDF 37EVA DB\n../_readthedocs/jupyter_execute/03-emotion-analysis_17_0.png\n../_readthedocs/jupyter_execute/03-emotion-analysis_17_1.png\n../_readthedocs/jupyter_execute/03-emotion-analysis_17_2.png\n../_readthedocs/jupyter_execute/03-emotion-analysis_17_3.png\n38 Chapter 11. EMOTION ANALYSISCHAPTER\nTWELVE\nIMAGE SEGMENTATION TUTORIAL\n12.1 Start EVA server\nWe are reusing the start server notebook for launching the EVA server.\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nFile \'00-start-eva-server.ipynb \'already there; not retrieving.\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server --port 8803 > eva.log 2>&1 &\n12.2 Download the Videos\n# # Getting the video files\n!wget -nc "https://www.dropbox.com/s/k00wge9exwkfxz6/ua_detrac.mp4?raw=1" -O ua_detrac.\n˓→mp4\nFile \'ua_detrac.mp4 \'already there; not retrieving.\n12.3 Load sample video from DAVIS dataset for analysis\ncursor.execute( \'DROP TABLE IF EXISTS VideoForSegmentation; \')\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute( \'LOAD VIDEO "ua_detrac.mp4" INTO VideoForSegmentation \')\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded VIDEO: 1\n39EVA DB\n12.4 Visualize Video\nfrom IPython.display import Video\nVideo("ua_detrac.mp4", embed= True)\n<IPython.core.display.Video object>\n12.5 Register Hugging Face Segmentation Model as an User-Defined\nFunction (UDF) in EVA\n### Using HuggingFace with EVA requires specifying the task\n### The task here is \'image-segmentation \'\n### The model is \'facebook/detr-resnet-50-panoptic \'\ncursor.execute("""CREATE UDF IF NOT EXISTS HFSegmentation\nTYPE HuggingFace\n\'task \' \'image-segmentation \'\n\'model \' \'facebook/detr-resnet-50-panoptic \'\n""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF HFSegmentation successfully added to the d...\n12.6 Run Image Segmentation on the video\ncursor.execute("""SELECT HFSegmentation(data)\nFROM VideoForSegmentation SAMPLE 5\nWHERE id < 20""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nhfsegmentation.score\n0 [0.906596, 0.989519, 0.960914, 0.923789, 0.960... \\\n1 [0.985118, 0.963139, 0.963819, 0.960939, 0.926...\n2 [0.989573, 0.900049, 0.966254, 0.96056, 0.9388...\n3 [0.913261, 0.949733, 0.943763, 0.98639, 0.9744...\nhfsegmentation.label\n0 [motorcycle, motorcycle, person, car, car, per... \\\n1 [motorcycle, person, car, car, person, bridge,...\n2 [motorcycle, person, person, car, car, car, pe...\n3 [truck, person, car, car, car, car, car, perso...\nhfsegmentation.mask\n0 [<PIL.Image.Image image mode=L size=960x540 at...\n(continues on next page)\n40 Chapter 12. Image Segmentation TutorialEVA DB\n(continued from previous page)\n1 [<PIL.Image.Image image mode=L size=960x540 at...\n2 [<PIL.Image.Image image mode=L size=960x540 at...\n3 [<PIL.Image.Image image mode=L size=960x540 at...\n12.7 Visualizing output of the Image Segmenter on the video\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport cv2\ndefget_color_mapping(all_labels):\nunique_labels = set(label forlabels inall_labels forlabel inlabels)\nnum_colors = len(unique_labels)\ncolormap = plt.colormaps["tab20"]\ncolors = [colormap(i % 20)[:3] foriinrange(num_colors)]\ncolors = [tuple(int(c * 255) forcincolor) forcolor incolors]\ncolor_mapping = {label: color forlabel, color inzip(unique_labels, colors)}\nreturn color_mapping\ndefannotate_single_frame(frame, segments, labels, color_mapping):\noverlay = np.zeros_like(frame)\n# Overlay segments\nformask, label inzip(segments, labels):\nmask_np = np.array(mask).astype(bool)\noverlay[mask_np] = color_mapping[label]\n# Combine original frame with overlay\nnew_frame = Image.blend(\nImage.fromarray(frame.astype(np.uint8)),\nImage.fromarray(overlay.astype(np.uint8)),\nalpha=0.5,\n)\nreturnnew_frame\ndefannotate_video(segmentations, input_video_path, output_video_path, model_name =\n˓→\'hfsegmentation \'):\nall_segments = segmentations[f \'{model_name }.mask \']\nall_labels = segmentations[f \'{model_name }.label \']\ncolor_mapping = get_color_mapping(all_labels)\nvcap = cv2.VideoCapture(input_video_path)\nwidth = int(vcap.get(3))\nheight = int(vcap.get(4))\nfps = vcap.get(5)\n(continues on next page)\n12.7. Visualizing output of the Image Segmenter on the video 41EVA DB\n(continued from previous page)\nfourcc = cv2.VideoWriter_fourcc( \'m\',\'p\',\'4\',\'v\')#codec\nvideo=cv2.VideoWriter(output_video_path, fourcc, fps, (width,height))\nframe_id = 0\nret, frame = vcap.read()\nwhileret andframe_id < len(all_segments):\nsegments = all_segments[frame_id]\nlabels = all_labels[frame_id]\nnew_frame = annotate_single_frame(frame, segments, labels, color_mapping)\nvideo.write(np.array(new_frame))\nifframe_id % 5 == 0:\nlegend_patches = [mpatches.Patch(color=np.array(color_mapping[label])/255,␣\n˓→label=label) forlabel inset(labels)]\nplt.imshow(new_frame)\nplt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc= \'upper left\n˓→\', borderaxespad=0.)\nplt.axis( \'off\')\nplt.tight_layout()\nplt.show()\nframe_id += 1\nret, frame = vcap.read()\nvideo.release()\nvcap.release()\nfrom ipywidgets import Video\ninput_path = \'ua_detrac.mp4 \'\noutput_path = \'video.mp4 \'\ndataframe = response.as_df()\nannotate_video(dataframe, input_path, output_path)\nVideo.from_file(output_path)\n../_readthedocs/jupyter_execute/07-object-segmentation-huggingface_16_0.png\nVideo(value=b \'\\x00\\x00\\x00\\x1cftypisom\\x00\\x00\\x02\\x00isomiso2mp41\\x00\\x00\\x00\\x08free\\\n˓→x00\\x01\\x90Z... \')\n42 Chapter 12. Image Segmentation TutorialEVA DB\n12.8 Dropping an User-Defined Function (UDF)\ncursor.execute("DROP UDF HFSegmentation;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF HFSegmentation successfully dropped\n12.8. Dropping an User-Defined Function (UDF) 43EVA DB\n44 Chapter 12. Image Segmentation TutorialCHAPTER\nTHIRTEEN\nCHATGPT TUTORIAL\n13.1 Start EVA server\nWe are reusing the start server notebook for launching the EVA server\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nFile ‘00-start-eva-server.ipynb’ already there; not retrieving.\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nnohup eva_server > eva.log 2>&1 &\n[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n13.2 Download News Video and ChatGPT UDF\n# Download News Video\n!wget -nc "https://www.dropbox.com/s/rfm1kds2mv77pca/russia_ukraine.mp4?dl=0" -O russia_\n˓→ukraine.mp4\n# Download ChatGPT UDF if needed\n!wget -nc https://raw.githubusercontent.com/georgia-tech-db/eva/master/eva/udfs/chatgpt.\n˓→py -O chatgpt.py\nFile ‘russia_ukraine.mp4’ already there; not retrieving.\n45EVA DB\nFile ‘chatgpt.py’ already there; not retrieving.\n13.3 Visualize Video\nfrom IPython.display import Video\nVideo("russia_ukraine.mp4", height=450, width=800, embed= True)\n<IPython.core.display.Video object>\n13.4 Set your OpenAI API key here\nfrom eva.configuration.configuration_manager import ConfigurationManager\nimport os\n# Assuming that the key is stored as an environment variable\nopen_ai_key = os.environ.get( \'OPENAI_KEY \')\nConfigurationManager().update_value("third_party", "openai_api_key", open_ai_key)\n# Drop the UDF if it already exists\ndrop_udf_query = f"DROP UDF IF EXISTS ChatGPT;"\ncursor.execute(drop_udf_query)\nresponse = cursor.fetch_all()\nresponse.as_df()\n# Register the ChatGPT UDF in EVA\ncreate_udf_query = f"""CREATE UDF ChatGPT\nIMPL \'chatgpt.py \'"""\ncursor.execute(create_udf_query)\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF ChatGPT successfully added to the database.\n13.5 Run the ChatGPT UDF\nsource/tutorials/chatgpt.png\n46 Chapter 13. ChatGPT TutorialEVA DB\n#load the video\ncursor.execute("LOAD VIDEO \'russia_ukraine.mp4 \'INTO VIDEOS;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded VIDEO: 1\n# Drop the Text Summarization UDF if needed\ncursor.execute("DROP UDF IF EXISTS SpeechRecognizer;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n# Create a Text Summarization UDF using Hugging Face\ntext_summarizer_udf_creation = """\nCREATE UDF SpeechRecognizer\nTYPE HuggingFace\n\'task \' \'automatic-speech-recognition \'\n\'model \' \'openai/whisper-base \';\n"""\ncursor.execute(text_summarizer_udf_creation)\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF SpeechRecognizer successfully added to the...\n# Drop the table if needed\ncursor.execute("DROP TABLE IF EXISTS TEXT_SUMMARY;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n# Create a materialized view of the text summarization output\ntext_summarization_query = """\nCREATE MATERIALIZED VIEW\nTEXT_SUMMARY(text) AS\nSELECT SpeechRecognizer(audio) FROM VIDEOS;\n"""\ncursor.execute(text_summarization_query)\nresponse = cursor.fetch_all()\nresponse.as_df()\nEmpty DataFrame\nColumns: []\nIndex: []\n# Run ChatGPT over the Text Summary extracted by Whisper\nchatgpt_udf = """\nSELECT ChatGPT( \'Is this video summary related to Ukraine russia war \',text)\nFROM TEXT_SUMMARY;\n(continues on next page)\n13.5. Run the ChatGPT UDF 47EVA DB\n(continued from previous page)\n"""\ncursor.execute(chatgpt_udf)\nresponse = cursor.fetch_all()\nresponse.as_df()\nchatgpt.response\n0 No, this video summary is not related to the U...\n1 Yes, the video summary is related to the Ukrai...\n13.6 Check if it works on an SNL Video\n# Download Entertainment Video\n!wget -nc "https://www.dropbox.com/s/u66im8jw2s1dmuw/snl.mp4?dl=0" -O snl.mp4\ncursor.execute("DROP TABLE IF EXISTS SNL_VIDEO;")\nresponse = cursor.fetch_all()\nresponse.as_df()\ncursor.execute("LOAD VIDEO \'snl.mp4 \'INTO SNL_VIDEO;")\nresponse = cursor.fetch_all()\nresponse.as_df()\nFile ‘snl.mp4’ already there; not retrieving.\n0\n0 Number of loaded VIDEO: 1\nfrom IPython.display import Video\nVideo("snl.mp4", height=450, width=800, embed= True)\n<IPython.core.display.Video object>\n# Drop the table if needed\ncursor.execute("DROP TABLE IF EXISTS SNL_TEXT_SUMMARY;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n# Create a materialized view of the text summarization output\ntext_summarization_query = """\nCREATE MATERIALIZED VIEW\nSNL_TEXT_SUMMARY(text) AS\nSELECT SpeechRecognizer(audio) FROM SNL_VIDEO;\n"""\ncursor.execute(text_summarization_query)\nresponse = cursor.fetch_all()\nresponse.as_df()\n48 Chapter 13. ChatGPT TutorialEVA DB\nEmpty DataFrame\nColumns: []\nIndex: []\n13.6.1 ChatGPT: Is this video summary related to Ukraine War?\n# Run ChatGPT over the Text Summary extracted by Whisper\nchatgpt_udf = """\nSELECT ChatGPT( \'Is this video summary related to Ukraine russia war \',text)\nFROM SNL_TEXT_SUMMARY;\n"""\ncursor.execute(chatgpt_udf)\nresponse = cursor.fetch_all()\nresponse.as_df()\nchatgpt.response\n0 No, this video summary is not related to the U...\n13.6.2 ChatGPT: Is this video summary related to a hospital?\n# Run ChatGPT over the Text Summary extracted by Whisper\nchatgpt_udf = """\nSELECT ChatGPT( \'Is this video summary related to a hospital \',text)\nFROM SNL_TEXT_SUMMARY;\n"""\ncursor.execute(chatgpt_udf)\nresponse = cursor.fetch_all()\nresponse.as_df()\nchatgpt.response\n0 Yes, the video summary is related to a hospita...\n13.6. Check if it works on an SNL Video 49EVA DB\n50 Chapter 13. ChatGPT TutorialCHAPTER\nFOURTEEN\nSIMILARITY SEARCH FOR MOTIF MINING\nInthistutorial,wedemonstratehowtoutilizethesimilarityfunctionalitytodiscoverimageswithsimilarmotifsfroma\ncollectionofRedditimages. Weemploytheclassic SIFTfeaturetoidentifyimageswithastrikinglysimilarappearance\n(image-level pipeline).\nAdditionally, we extend the pipeline by incorporating an object detection model, YOLO, in combination with the SIFT\nfeature. Thisenablesustoidentifyobjectswithintheimagesthatexhibitasimilarappearance(object-levelsimilarity).\nToillustratetheseamlessintegrationofdifferentvectorstores,weleveragethepowerofmultiplevectorstores,namely\nFAISSandQDRANT, within evadb. This demonstrates the ease with which you can utilize diverse vector stores to\nconstruct indexes, enhancing your similarity search experience.\n14.1 Start EVA server\nWe are reusing the start server notebook for launching the EVA server.\n!wget -nc "https://raw.githubusercontent.com/georgia-tech-db/eva/master/tutorials/00-\n˓→start-eva-server.ipynb"\n%run00-start-eva-server.ipynb\ncursor = connect_to_server()\nnohup eva_server > eva.log 2>&1 &\n14.2 Download reddit dataset\n!wget -nc https://www.dropbox.com/scl/fo/fcj6ojmii0gw92zg3jb2s/h \\?dl\\=1\\&rlkey \\\n˓→=j3kj1ox4yn5fhonw06v0pn7r9 -O reddit-images.zip\n!unzip -o reddit-images.zip -d reddit-images\nFile \'reddit-images.zip \'already there; not retrieving.\nArchive: reddit-images.zip\nwarning: stripped absolute path spec from /\nmapname: conversion of failed\nextracting: reddit-images/g348_d7jgzgf.jpg\nextracting: reddit-images/g348_d7jphyc.jpg\nextracting: reddit-images/g348_d7ju7dq.jpg\n(continues on next page)\n51EVA DB\n(continued from previous page)\nextracting: reddit-images/g348_d7jhhs3.jpg\nextracting: reddit-images/g1074_d4n1lmn.jpg\nextracting: reddit-images/g1074_d4mxztt.jpg\nextracting: reddit-images/g1074_d4n60oy.jpg\nextracting: reddit-images/g1074_d4n6fgs.jpg\nextracting: reddit-images/g1190_cln9xzr.jpg\nextracting: reddit-images/g1190_cln97xm.jpg\nextracting: reddit-images/g1190_clna260.jpg\nextracting: reddit-images/g1190_clna2x2.jpg\nextracting: reddit-images/g1190_clna91w.jpg\nextracting: reddit-images/g1190_clnad42.jpg\nextracting: reddit-images/g1190_clnajd7.jpg\nextracting: reddit-images/g1190_clnapoy.jpg\nextracting: reddit-images/g1190_clnarjl.jpg\nextracting: reddit-images/g1190_clnavnu.jpg\nextracting: reddit-images/g1190_clnbalu.jpg\nextracting: reddit-images/g1190_clnbf07.jpg\nextracting: reddit-images/g1190_clnc4uy.jpg\nextracting: reddit-images/g1190_clncot0.jpg\nextracting: reddit-images/g1190_clndsnu.jpg\nextracting: reddit-images/g1190_clnce4b.jpg\nextracting: reddit-images/g1209_ct65pvl.jpg\nextracting: reddit-images/g1209_ct66erw.jpg\nextracting: reddit-images/g1209_ct67oqk.jpg\nextracting: reddit-images/g1209_ct6a0g5.jpg\nextracting: reddit-images/g1209_ct6bf1n.jpg\nextracting: reddit-images/g1418_cj3o1h6.jpg\nextracting: reddit-images/g1418_cj3om3h.jpg\nextracting: reddit-images/g1418_cj3qysz.jpg\nextracting: reddit-images/g1418_cj3r4gw.jpg\nextracting: reddit-images/g1418_cj3z7jw.jpg\n14.3 Load all images into evadb\ncursor.execute("DROP TABLE IF EXISTS reddit_dataset;")\nresponse = cursor.fetch_all()\ncursor.execute("LOAD IMAGE \'reddit-images/*.jpg \'INTO reddit_dataset;")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Number of loaded IMAGE: 34\n52 Chapter 14. Similarity search for motif miningEVA DB\n14.4 Register a SIFT FeatureExtractor\nIt useskornialibrary to extract sift features for each image\n!pip install kornia --quiet\ncursor.execute("""CREATE UDF IF NOT EXISTS SiftFeatureExtractor\nIMPL \'../eva/udfs/sift_feature_extractor.py \'""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 UDF SiftFeatureExtractor successfully added to...\n# Keep track of which image gets the most votes\nfrom collections import Counter\nvote = Counter()\n14.5 Image-level similarity search pipeline.\nThispipelinecreatesonevectorperimage. Next,weshouldbreakdownstepshowwebuildtheindexandsearchsimilar\nvectors using the index.\n#1. Create index for the entire image\ncursor.execute("""CREATE INDEX reddit_sift_image_index\nON reddit_dataset (SiftFeatureExtractor(data))\nUSING FAISS""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Index reddit_sift_image_index successfully add...\n#2. Search similar vectors\ncursor.execute("""SELECT name FROM reddit_dataset ORDER BY\nSimilarity(\nSiftFeatureExtractor(Open( \'reddit-images/g1190_clna260.jpg \')),\nSiftFeatureExtractor(data)\n)\nLIMIT 5""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nreddit_dataset.name\n0 reddit-images/g1190_clna260.jpg\n1 reddit-images/g1190_clndsnu.jpg\n2 reddit-images/g1190_clna91w.jpg\n3 reddit-images/g1190_clnc4uy.jpg\n4 reddit-images/g1190_cln97xm.jpg\n14.4. Register a SIFT FeatureExtractor 53EVA DB\n#3. Update votes\nres_df = response.as_df()\nforiinrange(len(res_df)):\nvote[res_df["reddit_dataset.name"][i]] += 1\nprint(vote)\nCounter({ \'reddit-images/g1190_clna260.jpg \': 1, \'reddit-images/g1190_clndsnu.jpg \': 1,\n˓→\'reddit-images/g1190_clna91w.jpg \': 1, \'reddit-images/g1190_clnc4uy.jpg \': 1, \'reddit-\n˓→images/g1190_cln97xm.jpg \': 1})\n14.6 Object-level similarity search pipeline.\nThis pipeline detects objects within images and generates vectors exclusively from the cropped objects. The index is\nthenconstructedusingthesevectors. Toshowcasetheversatilityof evadb,weleverage Qdrantvectorstorespecifically\nfor building this index. This demonstrates how seamlessly you can leverage different vector stores within evadb.\n14.6.1 1. Extract all the object using Yolofrom the images\ncursor.execute("""\nCREATE MATERIALIZED VIEW reddit_object_table (name, data, bboxes,labels)\nAS SELECT name, data, bboxes, labels FROM reddit_dataset\nJOIN LATERAL UNNEST(Yolo(data)) AS Obj(labels, bboxes, scores)""")\nresponse = cursor.fetch_all()\nresponse.as_df()\nEmpty DataFrame\nColumns: []\nIndex: []\n14.6.2 2. Build an index on the feature vectors of the extracted objects\ncursor.execute("""CREATE INDEX reddit_sift_object_index\nON reddit_object_table (SiftFeatureExtractor(Crop(data, bboxes)))\nUSING QDRANT""")\nresponse = cursor.fetch_all()\nresponse.as_df()\n0\n0 Index reddit_sift_object_index successfully ad...\n# Create a cropped images (We are actively working on features to allow\n# us to not do this outside SQL)\ncursor.execute("LOAD IMAGE \'reddit-images/g1190_clna260.jpg \'INTO reddit_search_image_\n˓→dataset")\nresponse = cursor.fetch_all()\nprint(response.as_df())\n(continues on next page)\n54 Chapter 14. Similarity search for motif miningEVA DB\n(continued from previous page)\ncursor.execute("SELECT Yolo(data).bboxes FROM reddit_search_image_dataset")\nresponse = cursor.fetch_all()\nprint(response.as_df())\nimport cv2\nimport pathlib\nres_df = response.as_df()\nbboxes = res_df["yolo.bboxes"][0]\nimg = cv2.imread("reddit-images/g1190_clna260.jpg")\npathlib.Path("reddit-images/search-object/").mkdir(parents= True, exist_ok= True)\nfori, bbox inenumerate(bboxes):\nxmin, ymin, xmax, ymax = bbox\nxmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\ncropped_img = img[ymin:ymax, xmin:xmax]\ncv2.imwrite(f"reddit-images/search-object/search- {i}.jpg", cropped_img)\n0\n0 Number of loaded IMAGE: 1\nyolo.bboxes\n0 [[257.2467956542969, 256.8749084472656, 457.67...\n14.6.3 3. Retrieve using object-level similarity search\n#4.\nimport os\nforpath inos.listdir("reddit-images/search-object/"):\npath = "reddit-images/search-object/" + path\ncursor.execute(f"""SELECT name FROM reddit_object_table ORDER BY\nSimilarity(\nSiftFeatureExtractor(Open( \'{path }\')),\nSiftFeatureExtractor(data)\n)\nLIMIT 1""")\nresponse = cursor.fetch_all()\nres_df = response.as_df()\nforiinrange(len(res_df)):\nvote[res_df["reddit_object_table.name"][i]] += 0.5\nprint(response.as_df())\nreddit_object_table.name\n0 reddit-images/g1190_cln9xzr.jpg\nreddit_object_table.name\n0 reddit-images/g1190_cln9xzr.jpg\n(continues on next page)\n14.6. Object-level similarity search pipeline. 55EVA DB\n(continued from previous page)\nreddit_object_table.name\n0 reddit-images/g348_d7jgzgf.jpg\n14.7 Combine the scores from image level and object level similarity\nto show similar images\n# !pip install matplotlib\nimport matplotlib.pyplot as plt\n# Display top images\nvote_list = list(reversed(sorted([(path, count) forpath, count invote.items()],␣\n˓→key= lambdax: x[1])))\nimg_list = [path forpath, _ invote_list]\nfig, ax = plt.subplots(nrows=1, ncols=6, figsize=[18,10])\nax[0].imshow(cv2.imread("reddit-images/g1190_clna260.jpg"))\nax[0].set_title("Search")\nforiinrange(5):\naxi = ax[i + 1]\nimg = cv2.imread(img_list[i])\naxi.imshow(img)\naxi.set_title(f"Top- {i + 1 }")\nplt.show()\n../_readthedocs/jupyter_execute/11-similarity-search-for-motif-mining_26_0.png\n56 Chapter 14. Similarity search for motif miningCHAPTER\nFIFTEEN\nEVA QUERY LANGUAGE REFERENCE\nEVAQueryLanguage(EVAQL)isderivedfromSQL.Itistailoredforvideoanalytics. EVAQLallowsuserstoinvoke\ndeep learning models in the form of user-defined functions (UDFs).\nHereisanexamplewherewefirstdefineaUDFwrappingaroundtheFastRCNNobjectdetectionmodel. Wethenissue\na query with this function to detect objects.\n--- Create an user-defined function wrapping around FastRCNN ObjectDetector\nCREATEUDF IF NOT EXISTS FastRCNNObjectDetector\nINPUT(frame NDARRAY UINT8(3, ANYDIM, ANYDIM))\nOUTPUT(labels NDARRAY STR(ANYDIM), bboxes NDARRAY FLOAT32(ANYDIM, 4),\nscores NDARRAY FLOAT32(ANYDIM))\nTYPEClassification\nIMPL \'eva/udfs/fastrcnn_object_detector.py \';\n--- Use the function to retrieve frames that contain more than 3 cars\nSELECTidFROMMyVideo\nWHEREArrayCount(FastRCNNObjectDetector( data).label, \'car\') > 3\nORDER BY id;\nThis page presents a list of all the EVAQL statements that you can leverage in your Jupyter Notebooks.\n15.1 LOAD\n15.1.1 LOAD VIDEO FROM FILESYSTEM\nLOADVIDEO \'test_video.mp4 \'INTOMyVideo;\n•test_video.mp4 is the location of the video file in the filesystem on the client.\n•MyVideo is the name of the table in EVA where this video is loaded. Subsequent queries over the video must\nrefer to this table name.\nWhen a video is loaded, there is no need to specify the schema for the video table. EVA automatically generates the\nfollowing schema with two columns: idanddata, that correspond to the frame id and frame content (in Numpy\nformat).\n57EVA DB\n15.1.2 LOAD VIDEO FROM S3\nLOADVIDEO \'s3://bucket/dummy.avi \'INTOMyVideo;\nLOADVIDEO \'s3://bucket/eva_videos/*.mp4 \'INTOMyVideos;\nThe videos are downloaded to a directory that can be configured in the EVA configuration file under stor-\nage:s3_download_dir . The default directory is ~/.eva/s3_downloads .\n15.1.3 LOAD CSV\nToLOADa CSV file, we need to first specify the table schema.\nCREATE TABLE IF NOT EXISTS MyCSV (\nidINTEGER UNIQUE ,\nframe_id INTEGER,\nvideo_id INTEGER,\ndataset_name TEXT(30),\nlabel TEXT(30),\nbbox NDARRAY FLOAT32(4),\nobject_id INTEGER\n);\nLOADCSV \'test_metadata.csv \'INTOMyCSV;\n•test_metadata.csv needs to be loaded onto the server using LOADstatement.\n•The CSV file may contain additional columns. EVA will only load the columns listed in the defined schema.\n15.2 SELECT\n15.2.1 SELECT FRAMES WITH PREDICATES\nSearch for frames with a car\nSELECTid, frame\nFROMMyVideo\nWHERE[\'car\'] <@ FastRCNNObjectDetector(frame).labels\nORDER BY id;\nSearch frames with a pedestrian and a car\nSELECTid, frame\nFROMMyVideo\nWHERE[\'pedestrian \',\'car\'] <@ FastRCNNObjectDetector(frame).labels;\nSearch for frames containing greater than 3 cars\nSELECTidFROMMyVideo\nWHEREArrayCount(FastRCNNObjectDetector( data).label, \'car\') > 3\nORDER BY id;\n58 Chapter 15. EVA Query Language ReferenceEVA DB\n15.2.2 SELECT WITH MULTIPLE UDFS\nCompose multiple user-defined functions in a single query to construct semantically complex queries.\nSELECTid, bbox, EmotionDetector(Crop( data, bbox))\nFROMHAPPY JOIN LATERAL UNNEST (FaceDetector( data))ASFace(bbox, conf)\nWHEREid < 15;\n15.3 EXPLAIN\n15.3.1 EXPLAIN QUERY\nList the query plan associated with a EVAQL query\nAppendEXPLAIN in front of the query to retrieve the plan.\nEXPLAIN SELECT CLASS FROM TAIPAI;\n15.4 SHOW\n15.4.1 SHOW UDFS\nList the registered user-defined functions\nSHOWUDFS;\n15.5 CREATE\n15.5.1 CREATE TABLE\nTo create a table, specify the schema of the table.\nCREATE TABLE IF NOT EXISTS MyCSV (\nidINTEGER UNIQUE ,\nframe_id INTEGER,\nvideo_id INTEGER,\ndataset_name TEXT(30),\nlabel TEXT(30),\nbbox NDARRAY FLOAT32(4),\nobject_id INTEGER\n);\n15.3. EXPLAIN 59EVA DB\n15.5.2 CREATE UDF\nTo register an user-defined function, specify the implementation details of the UDF.\nCREATEUDF IF NOT EXISTS FastRCNNObjectDetector\nINPUT(frame NDARRAY UINT8(3, ANYDIM, ANYDIM))\nOUTPUT(labels NDARRAY STR(ANYDIM), bboxes NDARRAY FLOAT32(ANYDIM, 4),\nscores NDARRAY FLOAT32(ANYDIM))\nTYPEClassification\nIMPL \'eva/udfs/fastrcnn_object_detector.py \';\n15.5.3 CREATE MATERIALIZED VIEW\nTo create a view with materialized results – like the outputs of deep learning model, use the following template:\nCREATEMATERIALIZED VIEWUADETRAC_FastRCNN (id, labels) AS\nSELECTid, FastRCNNObjectDetector(frame).labels\nFROMUADETRAC\nWHEREid<5;\n15.6 DROP\n15.6.1 DROP TABLE\nDROP TABLE DETRACVideo;\n15.6.2 DROP UDF\nDROPUDF FastRCNNObjectDetector;\n15.7 INSERT\n15.7.1 TABLE MyVideo\nMyVideo Table schema\nCREATE TABLE MyVideo\n(id INTEGER,\ndataNDARRAY FLOAT32(ANYDIM));\n60 Chapter 15. EVA Query Language ReferenceEVA DB\n15.7.2 INSERT INTO TABLE\nInsert a tuple into a table.\nINSERT INTO MyVideo (id, data) VALUES\n(1,\n[[[40, 40, 40] , [40, 40, 40]],\n[[40, 40, 40] , [40, 40, 40]]]);\n15.8 DELETE\n15.8.1 DELETE INTO TABLE\nDelete a tuple from a table based on a predicate.\nDELETE FROM MyVideo WHEREid<10;\n15.9 RENAME\n15.9.1 RENAME TABLE\nRENAME TABLE MyVideo TOMyVideo1;\n15.8. DELETE 61EVA DB\n62 Chapter 15. EVA Query Language ReferenceCHAPTER\nSIXTEEN\nUSER-DEFINED FUNCTIONS\nThissectionprovidesanoverviewofhowyoucancreateanduseacustomuser-definedfunction(UDF)inyourqueries.\nFor example, you could write an UDF that wraps around your custom PyTorch model.\n16.1 Part 1: Writing a custom UDF\nDuring each step, use this UDF implementation as a reference.\n1. Create a new file under udfs/folder and give it a descriptive name. eg: yolo_object_detection.py .\nNote:UDFs packaged along with EVA are located inside the udfs folder.\n2. Create a Python class that inherits from PytorchClassifierAbstractUDF .\n•ThePytorchClassifierAbstractUDF is a parent class that defines and implements standard methods for model\ninference.\n•Thefunctionssetupandforwardshouldbeimplementedinyourchildclass. Thesefunctionscanbeimplemented\nwith the help of Decorators.\n16.2 Setup\nAn abstract method that must be implemented in your child class. The setup function can be used to initialize the\nparameters for executing the UDF. The parameters that need to be set are\n•cacheable: bool\n–True: Cache should be enabled. Cache will be automatically invalidated when the UDF changes.\n–False: cache should not be enabled.\n•udf_type: str\n–object_detection: UDFs for object detection.\n•batchable: bool\n–True: Batching should be enabled\n–False: Batching is disabled.\n63EVA DB\nThe custom setup operations for the UDF can be written inside the function in the child class. If there is no need for\nany custom logic, then you can just simply write “pass” in the function definition.\nExample of a Setup function\n@setup(cacheable= True, udf_type="object_detection", batchable= True)\ndefsetup(self, threshold=0.85):\n#custom setup function that is specific for the UDF\nself.threshold = threshold\nself.model = torch.hub.load("ultralytics/yolov5", "yolov5s", verbose= False)\n16.3 Forward\nAn abstract method that must be implemented in your UDF. The forward function receives the frames and runs the\ndeep learning model on the data. The logic for transforming the frames and running the models must be provided by\nyou. The arguments that need to be passed are\n•input_signatures: List[IOColumnArgument]\nDatatypesoftheinputstotheforwardfunctionmustbespecified. Ifnoconstraintsaregiven,thennovalidation\nis done for the inputs.\n•output_signatures: List[IOColumnArgument]\nDatatypesoftheoutputstotheforwardfunctionmustbespecified. Ifnoconstraintsaregiven,thennovalidation\nis done for the inputs.\nA sample forward function is given below\n@forward (\ninput_signatures=[\nPyTorchTensor(\nname="input_col",\nis_nullable= False,\ntype=NdArrayType.FLOAT32,\ndimensions=(1, 3, 540, 960),\n)\n],\noutput_signatures=[\nPandasDataframe(\ncolumns=["labels", "bboxes", "scores"],\ncolumn_types=[\nNdArrayType.STR,\nNdArrayType.FLOAT32,\nNdArrayType.FLOAT32,\n],\ncolumn_shapes=[( None,), ( None,), ( None,)],\n)\n],\n)\ndefforward(self, frames: Tensor) -> pd.DataFrame:\n#the custom logic for the UDF\noutcome = []\n(continues on next page)\n64 Chapter 16. User-Defined FunctionsEVA DB\n(continued from previous page)\nframes = torch.permute(frames, (0, 2, 3, 1))\npredictions = self.model([its.cpu().detach().numpy() * 255 forits inframes])\nforiinrange(frames.shape[0]):\nsingle_result = predictions.pandas().xyxy[i]\npred_class = single_result["name"].tolist()\npred_score = single_result["confidence"].tolist()\npred_boxes = single_result[["xmin", "ymin", "xmax", "ymax"]].apply(\nlambdax: list(x), axis=1\n)\noutcome.append(\n{"labels": pred_class, "bboxes": pred_boxes, "scores": pred_score}\n)\nreturnpd.DataFrame(outcome, columns=["labels", "bboxes", "scores"])\n16.4 Part 2: Registering and using the UDF in EVA Queries\nNow that you have implemented your UDF, we need to register it as a UDF in EVA. You can then use the UDF in any\nquery.\n1. Register the UDF with a query that follows this template:\nCREATE UDF [ IF NOT EXISTS ] <name> IMPL <path_to_implementation>;\nwhere,\n•<name> - specifies the unique identifier for the UDF.\n•<path_to_implementation> - specifies the path to the implementation class for the UDF\nHere, is an example query that registers a UDF that wraps around the ‘YoloObjectDetection’ model that\nperforms Object Detection.\nCREATEUDF YoloDecorators\nIMPL \'eva/udfs/decorators/yolo_object_detection_decorators.py \';\nA status of 0 in the response denotes the successful registration of this UDF.\n2. Now you can execute your UDF on any video:\nSELECTYoloDecorators( data)FROMMyVideo WHEREid < 5;\n3. You can drop the UDF when you no longer need it.\nDROPUDF IF EXISTS YoloDecorators;\n16.4. Part 2: Registering and using the UDF in EVA Queries 65EVA DB\n16.5 Ultralytics Models\nThis section provides an overview of how you can use out-of-the-box Ultralytics models in EVA.\n16.5.1 Creating YOLO Model\nTo create a YOLO UDF in EVA using Ultralytics models, use the following SQL command:\nCREATEUDF IF NOT EXISTS Yolo\nTYPEultralytics\n\'model \' \'yolov8m.pt \'\nYou can change the modelvalue to specify any other model supported by Ultralytics.\n16.5.2 Supported Models\nThe following models are currently supported by Ultralytics in EVA:\n•yolov8n.pt\n•yolov8s.pt\n•yolov8m.pt\n•yolov8l.pt\n•yolov8x.pt\nPlease refer to the Ultralytics documentation for more information about these models and their capabilities.\n16.5.3 Using Ultralytics Models with Other UDFs\nThis code block demonstrates how the YOLO model can be combined with other models such as Color and Dog-\nBreedClassifier to perform more specific and targeted object detection tasks. In this case, the goal is to find images of\nblack-colored Great Danes.\nThefirstqueryusesYOLOtodetectallimagesofdogswithblackcolor. The UNNESTfunctionisusedtosplittheoutput\nof theYoloUDF into individual rows, one for each object detected in the image. The ColorUDF is then applied to\nthecroppedportionoftheimagetoidentifythecolorofeachdetecteddogobject. The WHEREclausefilterstheresults\nto only include objects labeled as “dog” and with a color of “black”.\nSELECTid, bbox FROMdogs\nJOIN LATERAL UNNEST (Yolo( data))ASObj(label, bbox, score)\nWHEREObj.label = \'dog\'\nANDColor(Crop( data, bbox)) = \'black \';\nThe second query builds upon the first by further filtering the results to only include images of Great Danes. The\nDogBreedClassifier UDF is used to classify the cropped portion of the image as a Great Dane. The WHEREclause\nadds an additional condition to filter the results to only include objects labeled as “dog”, with a color of “black”, and\nclassified as a “great dane”.\nSELECTid, bbox FROMdogs\nJOIN LATERAL UNNEST (Yolo( data))ASObj(label, bbox, score)\nWHEREObj.label = \'dog\'\n(continues on next page)\n66 Chapter 16. User-Defined FunctionsEVA DB\n(continued from previous page)\nANDDogBreedClassifier(Crop( data, bbox)) = \'great dane \'\nANDColor(Crop( data, bbox)) = \'black \';\n16.6 HuggingFace Models\nThis section provides an overview of how you can use out-of-the-box HuggingFace models in EVA.\n16.6.1 Creating UDF from HuggingFace\nEVA supports UDFS similar to Pipelines in HuggingFace.\nCREATEUDF IF NOT EXISTS HFObjectDetector\nTYPEHuggingFace\n\'task \' \'object-detection \'\n\'model \' \'facebook / detr-resnet-50 \'\nEVA supports all arguments supported by HF pipelines. You can pass those using a key value format similar to task\nand model above.\n16.6.2 Supported Tasks\nEVA supports the following tasks from huggingface:\n•Audio Classification\n•Automatic Speech Recognition\n•Text Classification\n•Summarization\n•Text2Text Generation\n•Text Generation\n•Image Classification\n•Image Segmentation\n•Image-to-Text\n•Object Detection\n•Depth Estimation\n16.6. HuggingFace Models 67EVA DB\n16.7 OpenAI Models\nThis section provides an overview of how you can use OpenAI models in EVA.\n16.7.1 Chat Completion UDFs\nTo create a chat completion UDF in EVA, use the following SQL command:\nCREATEUDF IF NOT EXISTS OpenAIChatCompletion\nIMPL \'eva/udfs/openai_chat_completion_udf.py \'\n\'model \' \'gpt-3.5-turbo \'\nEVA supports the following models for chat completion task:\n•“gpt-4”\n•“gpt-4-0314”\n•“gpt-4-32k”\n•“gpt-4-32k-0314”\n•“gpt-3.5-turbo”\n•“gpt-3.5-turbo-0301”\nThechatcompletionUDFcanbecomposedininterestingwayswithotherUDFs. PleaserefertotheChatGPTnotebook\nfor an example of combining chat completion task with caption extraction and video summarization models from\nHugging Face and feeding it to chat completion to ask questions about the results.\n16.8 User-Defined Functions\nThissectionprovidesanoverviewofhowyoucancreateanduseacustomuser-definedfunction(UDF)inyourqueries.\nFor example, you could write a UDF that wraps around a PyTorch model.\n16.8.1 Part 1: Writing a Custom UDF\nDuring each step, use the UDF implementation as a reference.\n1. Create a new file under udfs/folder and give it a descriptive name, e.g., fastrcnn_object_detector.py .\nNote:UDFs packaged along with EVA are located inside the udfs folder.\n2. Create a Python class that inherits from PytorchClassifierAbstractUDF .\n•ThePytorchClassifierAbstractUDF is a parent class that defines and implements standard methods\nfor model inference.\n•Implement the setupandforward functions in your child class. These functions can be implemented\nwith the help of decorators.\n68 Chapter 16. User-Defined FunctionsEVA DB\n16.8.2 Setup\nAn abstract method that must be implemented in your child class. The setupfunction can be used to initialize the\nparameters for executing the UDF. The following parameters must be set:\n•cacheable : bool\n–True: Cache should be enabled. The cache will be automatically invalidated when the UDF changes.\n–False: Cache should not be enabled.\n•udf_type : str\n–object_detection : UDFs for object detection.\n•batchable : bool\n–True: Batching should be enabled.\n–False: Batching is disabled.\nThe custom setup operations for the UDF can be written inside the function in the child class. If no custom logic is\nrequired, then you can just write passin the function definition.\nExample of the setupfunction:\n@setup(cacheable= True, udf_type="object_detection", batchable= True)\ndefsetup(self, threshold=0.85):\nself.threshold = threshold\nself.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\nweights="COCO_V1", progress= False\n)\nself.model.eval()\nIn this instance, we have configured the cacheable andbatchable attributes to True. As a result, EVA will cache the\nUDF outputs and utilize batch processing for increased efficiency.\n16.8.3 Forward\nAnabstractmethodthatmustbeimplementedinyourchildclass. The forwardfunctionreceivestheframesandrunsthe\nDeep Learning model on the frames. The logic for transforming the frames and running the models must be provided\nby you. The arguments that need to be passed are:\n•input_signatures : List[IOColumnArgument]\nData types of the inputs to the forwardfunction must be specified. If no constraints are given, no validation is\ndone for the inputs.\n•output_signatures : List[IOColumnArgument]\nDatatypesoftheoutputsfromthe forwardfunctionmustbespecified. Ifnoconstraintsaregiven,novalidation\nis done for the inputs.\nA sample forward function is given below:\n@forward (\ninput_signatures=[\nPyTorchTensor(\nname="input_col",\nis_nullable= False,\n(continues on next page)\n16.8. User-Defined Functions 69EVA DB\n(continued from previous page)\ntype=NdArrayType.FLOAT32,\ndimensions=(1, 3, 540, 960),\n)\n],\noutput_signatures=[\nPandasDataframe(\ncolumns=["labels", "bboxes", "scores"],\ncolumn_types=[\nNdArrayType.STR,\nNdArrayType.FLOAT32,\nNdArrayType.FLOAT32,\n],\ncolumn_shapes=[( None,), ( None,), ( None,)],\n)\n],\n)\ndefforward(self, frames: Tensor) -> pd.DataFrame:\npredictions = self.model(frames)\noutcome = []\nforprediction inpredictions:\npred_class = [\nstr(self.labels[i]) foriinlist(self.as_numpy(prediction["labels"]))\n]\npred_boxes = [\n[i[0], i[1], i[2], i[3]]\nforiinlist(self.as_numpy(prediction["boxes"]))\n]\nIn this instance, the forward function takes a PyTorch tensor of Float32 type with a shape of (1, 3, 540, 960) as input.\nThe resulting output is a pandas dataframe with 3 columns, namely “labels”, “bboxes”, and “scores”, and of string,\nfloat32, and float32 types respectively.\n16.8.4 Part 2: Registering and using the UDF in queries\nNow that you have implemented your UDF we need to register it in EVA. You can then use the function in any query.\nRegister the UDF in EVA\nCREATEUDF [ IF NOT EXISTS ] <name>\nIMPL <implementation_path>;\nname- specifies the unique identifier for the UDF.\nimplementation_path - specifies the path to the implementation class for the UDF\nHere, is an example query that registers a UDF that wraps around the fasterrcnn_resnet50_fpn model that per-\nforms Object Detection.\nCREATEUDF FastRCNNObjectDetector\nIMPL \'eva/udfs/fastrcnn_object_detector.py \';\n70 Chapter 16. User-Defined FunctionsEVA DB\nCall registered UDF in a query\nSELECTFastRCNNObjectDetector( data)FROMMyVideo WHEREid < 5;\nDrop the UDF\nDROPUDF IF EXISTS FastRCNNObjectDetector;\n16.8. User-Defined Functions 71EVA DB\n72 Chapter 16. User-Defined FunctionsCHAPTER\nSEVENTEEN\nIO DESCRIPTORS\nEVA supports three key data types. The inputs and outputs of the user-defined functions (UDFs) must be of one of\nthese types.\n17.1 NumpyArray\nUsed when the inputs or outputs of the UDF is of type Numpy Array.\n17.2 Parameters\nname (str): name of the numpy array.\nis_nullable ( bool): boolean value indicating if the numpy array can be NULL.\ntype (NdArrayType ): data type of all the elements in the numpy array. The available types can be found in\neva/catalog/catalog_type.py in the class NdArrayType\ndimensions( Tuple(int) ): shape of the numpy array\nfrom eva.catalog.catalog_type import NdArrayType\nNumpyArray(\nname="input_arr",\nis_nullable= False,\ntype=NdArrayType.INT32,\ndimensions=(2, 2),\n)\n17.3 PyTorchTensor\nname (str): name of the pytorch tensor.\nis_nullable ( bool): boolean value indicating if the pytorch tensor can be NULL.\ntype (NdArrayType ): data type of elements in the pytorch tensor. The available types can be found in\neva/catalog/catalog_type.py in class NdArrayType\ndimensions( Tuple(int) ): shape of the numpy array\n73EVA DB\nfrom eva.catalog.catalog_type import NdArrayType\nPyTorchTensor(\nname="input_arr",\nis_nullable= False,\ntype=NdArrayType.INT32,\ndimensions=(2, 2),\n)\n17.4 PandasDataframe\ncolumns ( List[str]): list of strings that represent the expected column names in the pandas dataframe that is returned\nfrom the UDF.\ncolumn_types ( NdArrayType ): expected datatype of the column in the pandas dataframe returned from the UDF. The\nNdArrayType class is inherited from eva.catalog.catalog_type.\ncolumn_shapes ( List[tuples] ): list of tuples that represent the expected shapes of columns in the pandas dataframe\nreturned from the UDF.\nPandasDataframe(\ncolumns=["labels", "bboxes", "scores"],\ncolumn_types=[\nNdArrayType.STR,\nNdArrayType.FLOAT32,\nNdArrayType.FLOAT32,\n],\ncolumn_shapes=[( None,), ( None,), ( None,)],\n)\n74 Chapter 17. IO DescriptorsCHAPTER\nEIGHTEEN\nCONFIGURE GPU\n1. QueriesinEVAusedeeplearningmodelsthatrunmuchfasteronaGPUasopposedtoaCPU.Ifyourworkstation\nhasaGPU,youcanconfigureEVAtousetheGPUduringqueryexecution. Usethefollowingcommandtocheck\nyour hardware capabilities:\nubuntu-drivers devices\nnvidia-smi\nAvalidoutputfromthecommandindicatesthatyourGPUisconfiguredandreadytouse. Ifnot,youwillneedtoinstall\nthe appropriate GPU driver. This page provides a step-by-step guide on installing and configuring the GPU driver in\nthe Ubuntu Operating System.\n•When installing an NVIDIA driver, ensure that the version of the GPU driver is correct to avoid compatibility\nissues.\n•WheninstallingcuDNN,youwillneedtocreateanaccountandensurethatyougetthecorrect debfilesforyour\noperating system and architecture.\n2. You can run the following code in a Jupyter notebook to verify that your GPU is detected by PyTorch:\nimport torch\ndevice = torch.device( \'cuda:0 \'iftorch.cuda.is_available() else \'cpu\')\nprint(device)\nOutput of cuda:0indicates the presence of a GPU. 0 indicates the index of the GPU in system. If you have multiple\nGPUs on your workstation, the index must be updated accordingly.\n3. Now configure the executor section in eva.yml as follows:\nexecutor :\ngpus: {\'127.0.1.1 \': [0]}\nHere,127.0.1.1 is the loopback address on which the EVA server is running. 0 refers to the GPU index to be used.\n75EVA DB\n76 Chapter 18. Configure GPUCHAPTER\nNINETEEN\nEVA INTERNALS\n19.1 Path of a Query\nThe following coderepresents a sequence of operations thatcan be used to execute aquery in a evaql database. found\nin eva/server/command_handler.py\nParse the query using the Parser() function provided by the evaql library. The result of this step will be a\nparsed representation of the query in the form of an abstract syntax tree (AST).\nstmt = Parser().parse(query)[0]\nBind the parsed AST to a statement context using the StatementBinder() function. This step resolves references to\nschema objects and performs other semantic checks on the query.\nStatementBinder(StatementBinderContext()).bind(stmt)\nConverttheboundASTtoalogicalplanusingtheStatementToPlanConvertor()function. Thisstepgeneratesalogical\nplan that specifies the sequence of operations needed to execute the query.\nl_plan = StatementToPlanConvertor().visit(stmt)\nGenerateaphysicalplanfromthelogicalplanusingtheplan_generator.build()function. Thisstepoptimizesthelogical\nplan and generates a physical plan that specifies how the query will be executed.\np_plan = plan_generator.build(l_plan)\nExecutethephysicalplanusingthePlanExecutor()function. Thisstepretrievesthedatafromthedatabaseandproduces\nthe final output of the query.\noutput = PlanExecutor(p_plan).execute_plan()\nOverall,thissequenceofoperationsrepresentsthepathofqueryexecutioninaevaqldatabase,fromparsingthequery\nto producing the final output.\n77EVA DB\n19.2 Topics\n19.2.1 Catalog\nCatalog Manager\nExplanation for developers on how to use the eva catalog_manager.\nCatalogManager class that provides a set of services to interact with a database that stores metadata about tables,\ncolumns,anduser-definedfunctions(UDFs). Informationlikewhatisthedatatypeinacertaincolumninatable,type\nof a table, its name, etc.. It contains functions to get, insert and delete catalog entries for Tables, UDFs, UDF IOs,\nColumns and Indexes.\nThis data is stored in the eva_catalog.db file which can be found in ~/.eva/<version>/ folder.\nCatalog manager currently has 5 services in it:\nTableCatalogService()\nColumnCatalogService()\nUdfCatalogService()\nUdfIOCatalogService()\nIndexCatalogService()\nCatalog Services\nThis class provides functionality related to a table catalog, including inserting, getting, deleting, and renaming table\nentries, as well as retrieving all entries. e.g. the TableCatalogService contains code to get, insert and delete a table.\nCatalog Models\nThese contain the data model that is used by the catalog services. Each model represents a table in the underlying\ndatabase.\n78 Chapter 19. EVA InternalsCHAPTER\nTWENTY\nCONTRIBUTING\nWe welcome all kinds of contributions to EVA.\n•Code reviews\n•Improving documentation\n•Tutorials and applications\n•New features\n20.1 Setting up the Development Environment\nFirst, you will need to checkout the repository from GitHub and build EVA from the source. Follow the following\ninstructions to build EVA locally. We recommend using a virtual environment and the pip package manager.\ngit clone https://github.com/georgia-tech-db/eva.git && cd eva\npython3 -m venv test_eva_db # create a virtual environment\nsource test_eva_db/bin/activate # activate the virtual environment\npip install --upgrade pip # upgrade pip\npip install -e ".[dev]" # build and install the EVA package\nbash script/test/test.sh # run the eva EVA suite\nAfter installing the package locally, you can make changes and run the test cases to check their impact.\npip install . # reinstall EVA package to include local changes\npkill -9 eva_server # kill running EVA server (if any)\neva_server& # launch EVA server with newly installed package\n20.2 Testing\nCheck if your local changes broke any unit or integration tests by running the following script:\nbash script/test/test.sh\nIf you want to run a specific test file, use the following command.\npython -m pytest test/integration_tests/test_select_executor.py\nUse the following command to run a specific test case within a specific test file.\n79EVA DB\npython -m pytest test/integration_tests/test_select_executor.py -k \'test_should_load_and_\n˓→select_in_table \'\n20.3 Submitting a Contribution\nFollow the following steps to contribute to EVA:\n•Merge the most recent changes from the master branch\ngit remote add origin git@github.com:georgia-tech-db/eva.git\ngit pull . origin/master\n•Run thetest script to ensure that all the test cases pass.\n•If you are adding a new EVAQL command, add an illustrative example usage in the documentation.\n•Run the following command to ensure that code is properly formatted.\npython script/formatting/formatter.py\n20.4 Code Style\nWe use the black code style for formatting the Python code. For docstrings and documentation, we use Google Pydoc\nformat.\ndeffunction_with_types_in_docstring(param1, param2) -> bool:\n"""Example function with types documented in the docstring.\nAdditional explanatory text can be added in paragraphs.\nArgs:\nparam1 (int): The first parameter.\nparam2 (str): The second parameter.\nReturns:\nbool: The return value. True for success, False otherwise.\n20.5 Debugging\nWe recommend using Visual Studio Code with a debugger for developing EVA. Here are the steps for setting up the\ndevelopment environment:\n1. Install the Python extension in Visual Studio Code.\n2. Install the Python Test Explorer extension.\n3. Follow these instructions to run a particular test case from the file: Getting started.\n80 Chapter 20. ContributingEVA DB\n20.5. Debugging 81EVA DB\n20.6 Architecture Diagram\n20.7 Troubleshooting\nIf the test suite fails with a PermissionDenied exception, update the path_prefix attribute under the storagesection in\nthe EVA configuration file ( ~/.eva/eva.yml ) to a directory where you have write privileges.\n82 Chapter 20. ContributingCHAPTER\nTWENTYONE\nDEBUGGING\nWerecommendVisualStudioCodewithadebuggerfordebuggingEVA.Thistutorialpresentsadetailedstep-by-step\nprocess of using the debugger.\n21.1 Setup debugger\n1. Install the Python extension in Visual Studio Code.\n2. Install the Python Test Explorer extension.\n3. Follow these instructions to run a particular test case from the file: Getting started.\n83EVA DB\n21.2 Alternative: Manually Setup Debugger for EVA\nWhen you press the debug icon, you will be given an option to create a launch.json file.\nWhilecreatingtheJSONfile,youwillbepromptedtoselecttheenvironmenttobeused. Selectthepythonenvironment\nfromthecommand palette atthetop. IfthePythonenvironmentcannotbeseeninthedrop-downmenu,tryinstalling\nthe python extension, and repeat the process.\nOnce you select the python environment, a launch.json file will be created with the default configurations set to\ndebug a simple .py file.\nMore configurations can further be added to the file, to modify the environment variables or to debug an entire folder\nor workspace directory. Use the following configuration in the JSON file:\n{\n"version" : "0.2.0",\n"configurations" : [\n{\n"name": "Python: test_pytorch.py",\n"type": "python",\n"request" : "launch",\n"program" : "${workspaceFolder}/test/integration_tests/test_pytorch.py",\n"console" : "integratedTerminal",\n"cwd": "${workspaceFolder}",\n"env": {"PYTHONPATH" : "${workspaceRoot}"}\n(continues on next page)\n84 Chapter 21. DebuggingEVA DB\n(continued from previous page)\n}\n]\n}\nYou can modify the fields of the above JSON file as follows:\nname: It is the reader-friendly name to appear in the Debug launch configuration dropdown.\ntype: The type of debugger to use for this launch configuration.\nprogram: The executable or file to run when launching the debugger. In the above example,\ntest_integration.py will be executed by the debugger.\nenv: Hereyouspecifytheenvironmentvariables. Intheaboveexample,thepathforthecondaenvironment\nfor Eva has been specified.\nUsing these configuration variables, you can run the debugger both locally as well as on a remote server.\n21.2. Alternative: Manually Setup Debugger for EVA 85EVA DB\n86 Chapter 21. DebuggingCHAPTER\nTWENTYTWO\nEXTENDING EVA\nThisdocumentdetailsthestepsinvolvedinaddingsupportforanewoperator(orcommand)inEVA.Weillustratethe\nprocess using a DDL command.\n22.1 Command Handler\nAn input query string is handled by Parser,StatementTOPlanConverter ,PlanGenerator , andPlanExecutor . We\ndiscuss each part separately.\ndefexecute_query(query) -> Iterator[Batch]:\n"""\nExecute the query and return a result generator.\n"""\n#1. parser\nstmt = Parser().parse(query)[0]\n#2. statement to logical plan\nl_plan = StatementToPlanConverter().visit(stmt)\n#3. logical to physical plan\np_plan = PlanGenerator().build(l_plan)\n#4. parser\nreturnPlanExecutor(p_plan).execute_plan()\n22.2 1. Parser\nThe parser firstly generate syntax tree from the input string, and then transform syntax tree into statement .\nThe first part of Parser is build from a LARK grammar file.\n22.2.1 parser/eva\n•eva.lark - add keywords(eg. CREATE, TABLE) under Common Keywords\n–Add new grammar rule (eg. create_table)\n–Write a new grammar, for example:\ncreate_table: CREATE TABLE if_not_exists? table_name create_definitions\nThe second part of parser is implemented as parser visitor .\n87EVA DB\n22.2.2 parser/lark_visitor\n•_[cmd]_statement.py - eg. class CreateTable(evaql_parserVisitor)\n–Write functions to transform each input data from syntax tree to desired type. (eg. transform Column\ninformation into a list of ColumnDefinition)\n–Write a function to construct [cmd]Statement and return it.\n•__init__.py - import_[cmd]_statement.py and add its class to ParserVisitor ’s parent class.\nfrom src.parser.parser_visitor._create_statement import CreateTable\nclass ParserVisitor (CommonClauses, CreateTable, Expressions,\nFunctions, Insert, Select, TableSources,\nLoad, Upload):\n22.2.3 parser/\n•[cmd]_statement.py - class [cmd]Statement. Its constructor is called in _[cmd]_statement.py\n•types.py - register new StatementType\n22.3 2. Statement To Plan Converter\nThe part transforms the statement into corresponding logical plan.\n22.3.1 Optimizer\n•operators.py\n–Define class Logical[cmd], which is the logical node for the specific type of command.\nclass LogicalCreate (Operator):\ndef__init__(self, video: TableRef, column_list: List[DataFrameColumn], if_not_\n˓→exists: bool = False, children= None):\nsuper().__init__(OperatorType.LOGICALCREATE, children)\nself._video = video\nself._column_list = column_list\nself._if_not_exists = if_not_exists\n# ...\n–Register new operator type to class OperatorType , Notice that must add it before LOGICALDELIM-\nITER!!!\n•statement_to_opr_convertor.py\n–import resource\nfrom src.optimizer.operators import LogicalCreate\nfrom src.parser.rename_statement import CreateTableStatement\n–implement visit_[cmd]() function, which converts statement to operator\n88 Chapter 22. Extending EVAEVA DB\n# May need to convert the statement into another data type.\n# The new data type is usable for executing command.\n# For example, column_list -> column_metadata_list\ndefvisit_create(self, statement: AbstractStatement):\nvideo_ref = statement.table_ref\nifvideo_ref is None:\nLoggingManager().log("Missing Table Name In Create Statement",\nLoggingLevel.ERROR)\nif_not_exists = statement.if_not_exists\ncolumn_metadata_list = create_column_metadata(statement.column_list)\ncreate_opr = LogicalCreate(\nvideo_ref, column_metadata_list, if_not_exists)\nself._plan = create_opr\n–modify visit function to call the right visit_[cmd] function\ndefvisit(self, statement: AbstractStatement):\nifisinstance(statement, SelectStatement):\nself.visit_select(statement)\n#...\nelifisinstance(statement, CreateTableStatement):\nself.visit_create(statement)\nreturnself._plan\n22.4 3. Plan Generator\nTheparttransformedlogicalplantophysicalplan. Themodifiedfilesarestoredunder Optimizer andPlannerfolders.\n22.4.1 plan_nodes/\n•[cmd]_plan.py - class [cmd]Plan, which stored information required for rename table.\nclass CreatePlan (AbstractPlan):\ndef__init__(self, video_ref: TableRef,\ncolumn_list: List[DataFrameColumn],\nif_not_exists: bool = False):\nsuper().__init__(PlanOprType.CREATE)\nself._video_ref = video_ref\nself._column_list = column_list\nself._if_not_exists = if_not_exists\n#...\n•types.py - register new plan operator type to PlanOprType\n22.4. 3. Plan Generator 89EVA DB\n22.4.2 optimizer/rules\n•rules.py -\n–Import operators\n–Register new ruletype to RuleType andPromise (place itbefore IMPLEMENTATION_DELIMITER\n!!)\n–implement class Logical[cmd]ToPhysical , its member function apply() will construct a correspond-\ning[cmd]Plan object.\nclass LogicalCreateToPhysical (Rule):\ndef__init__(self):\npattern = Pattern(OperatorType.LOGICALCREATE)\nsuper().__init__(RuleType.LOGICAL_CREATE_TO_PHYSICAL, pattern)\ndefpromise(self):\nreturnPromise.LOGICAL_CREATE_TO_PHYSICAL\ndefcheck(self, before: Operator, context: OptimizerContext):\nreturn True\ndefapply(self, before: LogicalCreate, context: OptimizerContext):\nafter = CreatePlan(before.video, before.column_list, before.if_not_exists)\nreturnafter\n•rules_base.py -\n–Register new ruletype to RuleType andPromise (place itbefore IMPLEMENTATION_DELIMITER\n!!)\n•rules_manager.py -\n–Import rules created in rules.py\n–Add imported logical to physical rules to self._implementation_rules\n22.5 4. Plan Executor\nPlanExecutor uses data stored in physical plan to run the command.\n22.5.1 executor/\n•[cmd]_executor.py - implement an executor that make changes in catalog,metadata , orstorage engine to\nrun the command.\n–May need to create helper function in CatalogManager, DatasetService, DataFrameMetadata, etc.\nclass CreateExecutor (AbstractExecutor):\ndefexec(self):\nif(self.node.if_not_exists):\n# check catalog if we already have this table\nreturn\n(continues on next page)\n90 Chapter 22. Extending EVAEVA DB\n(continued from previous page)\ntable_name = self.node.video_ref.table_info.table_name\nfile_url = str(generate_file_path(table_name))\nmetadata = CatalogManager().create_metadata(table_name, file_url, self.node.\n˓→column_list)\nStorageEngine.create(table=metadata)\n22.6 Additional Notes\nKey data structures in EVA:\n•Catalog: Records DataFrameMetadata for all tables.\n–data stored in DataFrameMetadata: name,file_url ,identifier_id ,schema\n∗file_url - used to access the real table in storage engine.\n–FortheRENAMEtablecommand,weusethe old_table_name toaccessthecorrespondingentryinmetadata\ntable, and the modified name of the table.\n•Storage Engine :\n–API is defined in src/storage , currently only supports create, read, write.\n22.6. Additional Notes 91EVA DB\n92 Chapter 22. Extending EVACHAPTER\nTWENTYTHREE\nEVA RELEASE GUIDE\n23.1 Part 1: Before You Start\nMakesureyouhavePyPIaccountwithmaintaineraccesstotheEVAproject. Createa.pypircinyourhomedirectory.\nIt should look like this:\n[distutils]\nindex-servers =\npypi\npypitest\n[pypi]\nusername=YOUR_USERNAME\npassword=YOUR_PASSWORD\nThen runchmod 600 ./.pypirc so that only you can read/write the file.\n23.2 Part 2: Release Steps\n1. Ensure that you’re in the top-level evadirectory.\n2. Ensure that your branch is in sync with the masterbranch:\n$ git pull origin master\n3. Add a new entry in the Changelog for the release.\n## [0.0.6]\n### [Breaking Changes]\n### [Added]\n### [Changed]\n### [Deprecated]\n### [Removed]\nMake sure CHANGELOG.md is up to date for the release: compare against PRs merged since the last release.\n4. Update version to, e.g. 0.0.6(remove the +devlabel) ineva/version.py .\n5. Commit these changes and create a PR:\n93EVA DB\ngit checkout -b release-v0.0.6\ngit add . -u\ngit commit -m "[RELEASE]: v0.0.6"\ngit push --set-upstream origin release-v0.0.6\n6. Once the PR is approved, merge it and pull master locally.\n7. Tag the release:\ngit tag -a v0.0.6 -m "v0.0.6 release"\ngit push origin v0.0.6\n8. Build the source and wheel distributions:\nrm -rf dist build # clean old builds & distributions\npython3 setup.py sdist # create a source distribution\npython3 setup.py bdist_wheel # create a universal wheel\n9. Check that everything looks correct by installing the wheel locally and checking the version:\npython3 -m venv test_evadb # create a virtualenv for testing\nsource test_evadb/bin/activate # activate virtualenv\npython3 -m pip install dist/evadb-0.9.1-py3-none-any.whl\npython3 -c "import eva; print(eva.__version__)"\n10. Publish to PyPI\npip install twine # if not installed\ntwine upload dist/* -r pypi\n11. APRisautomaticallysubmitted(thiswilltakeafewhours)on[ conda-forge/eva-feedstock ](https://github.com/\nconda-forge/eva-feedstock) to update the version. * A maintainer needs to accept and merge those changes.\n12. Create a new release on Github. * Input the recently-created Tag Version: v0.0.6* Copy the release notes in\nCHANGELOG.md to the GitHub tag. * Attach the resulting binaries in ( dist/evadb-x.x.x.* ) to the release. *\nPublish the release.\n13. Update version to, e.g. 0.9.1+dev ineva/version.py .\n14. Add a new changelog entry for the unreleased version in CHANGELOG.md :\n## [Unreleased]\n### [Breaking Changes]\n### [Added]\n### [Changed]\n### [Deprecated]\n### [Removed]\n15. Commit these changes and create a PR:\ngit checkout -b bump-v0.9.1+dev\ngit add . -u\ngit commit -m "[BUMP]: v0.9.1+dev"\ngit push --set-upstream origin bump-v0.9.1+dev\n16. Add the new tag to the EVA project on ReadTheDocs,\n94 Chapter 23. EVA Release GuideEVA DB\n•Trigger a build for main to pull new tags.\n•Go to the Versions tab, andActivate the new tag.\n•Go to Admin/Advanced to set this tag as the new default version.\n•InOverview , make sure a build is triggered:\n–For the tag v0.9.1\n–Forlatest\nCredits: Snorkel\n23.2. Part 2: Release Steps 95EVA DB\n96 Chapter 23. EVA Release GuideCHAPTER\nTWENTYFOUR\nPACKAGING\nThis section describes practices to follow when packaging your own models or datasets to be used along\nwith EVA.\n24.1 Models\nPlease follow the following steps to package models:\n•Createafolderwithadescriptivename. ThisfoldernamewillbeusedbytheUDFthatisinvokingyourmodel.\n•Place all files used by the UDF inside this folder. These are typically:\n–Model weights (The .pt files that contain the actual weights)\n–Model architectures (The .pt files that contain model architecture information)\n–Label files (Extra files that are used in the process of model inference for outputting labels.)\n–Other config files (Any other config files required for model inference)\n•Zip this folder.\n•Upload the zipped folder to this link inside the models folder.\n24.2 Datasets\nPlease follow the following steps to package datasets:\n•Create a folder for your dataset and give it a descriptive name.\n•This dataset folder should contain 2 sub-folders named ‘info’ and ‘videos’. For each video entry in the videos\nfolder, there should be a corresponding CSV file in the info folder with the same name. The structure should\nlook like:\n97EVA DB\n•The videos folder should contain the raw videos in a standard format like mp4 or mov.\n•The info folder should contain the meta information corresponding to each video in CSV format. Each row of\nthisCSVfileshouldcorrespondto1uniqueobjectinagivenframe. PleasemakesurethecolumnsinyourCSV\nfile exactly match to these names. Here is a snapshot of a sample CSV file:\nThe columns represent the following:\n–id-(Integer)Autoincrementingindexthatisuniqueacrossallfiles(SincetheCSVfilesare\nwritten to the same meta table, we want it to be unique across all files)\n–frame_id - (Integer) id of the frame this row corresponds to.\n–video_id - (Integer) id of the video this file corresponds to.\n–dataset_name - (String) Name of the dataset (should match the folder name)\n–label - (String) label of the object this row corresponds to.\n–bbox-(String)commaseparatedfloatvaluesrepresentingx1,y1,x2,y2(topleftandbottom\nright) coordinates of the bounding box\n98 Chapter 24. PackagingEVA DB\n–object_id - (Integer) unique id for the object corresponding to this row.\n•Zip this folder.\n•Upload the zipped folder to this link inside the datasets folder.\nNote: Inthefuture,willprovideutilityscriptsalongwithEVAtodownloadmodelsanddatasetseasilyandplacethem\nin the appropriate locations.\n24.2. Datasets 99', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n')]